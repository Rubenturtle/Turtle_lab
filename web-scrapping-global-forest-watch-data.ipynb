{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping - Global Forest Watch data\n",
    "- We create the output folder in case it does not exists in the \"Downloads folder\"\n",
    "- This script iterates over the dataset repository\n",
    "- Gets the link headers \"xxxx.tif\"\n",
    "- Since they don't contain all the strcuture of the sentence, we build the input link with the link header\n",
    "- Pointing to the download created folder we download them one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website to scrape\n",
    "# url = \"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/treecover2000.txt\"\n",
    "# url = \"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/gain.txt\"\n",
    "url = \"https://storage.googleapis.com/earthenginepartners-hansen/GFC-2022-v1.10/lossyear.txt\"\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# Find all links in the webpage\n",
    "links = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder inside \"Downloads\" to store downloaded files\n",
    "def set_output_folder(output_folder_name):\n",
    "    output_admin_folder = \"Downloads\"\n",
    "    download_folder = os.path.join(os.path.expanduser('~'), output_admin_folder, output_folder_name)\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    print(\"Available the folder: \" + output_folder_name)\n",
    "    return download_folder\n",
    "\n",
    "# Get the file links\n",
    "def make_raster_link_list(links):\n",
    "    web_file_list = []\n",
    "    for link in links:\n",
    "        href = link.get('href') # the name of the file\n",
    "        if href and href.endswith('.tif') and href.startswith('treecover'): # We would have to edit this\n",
    "            web_file_list.append(href)\n",
    "    print(len(web_file_list))\n",
    "    return web_file_list\n",
    "\n",
    "def download_file(file_url, file_location):\n",
    "    retries = 3\n",
    "    while retries > 0:\n",
    "        failed_layers = []\n",
    "        try:\n",
    "            # Here we open the link and download the file\n",
    "            with open(file_location, 'wb') as f:\n",
    "                response = requests.get(file_url)\n",
    "                f.write(response.content)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed for {file_location}. Retrying...\")\n",
    "            retries -= 1\n",
    "            if retries == 0:\n",
    "                print(f\"Failed to download {file_location} after 3 attempts. Error: {str(e)}\")\n",
    "                failed_layers.append(file_location)\n",
    "                break\n",
    "    if failed_layers: # If there are failed layers, show them\n",
    "        print(\"the next files were not downloaded\")\n",
    "        for file in failed_layers:\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available the folder: global_forest_watch_data_loss\n"
     ]
    }
   ],
   "source": [
    "output_folder_name = \"global_forest_watch_data_loss\"\n",
    "# Create the folder\n",
    "download_folder = set_output_folder(output_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_file_list = []\n",
    "if not links:\n",
    "     # The input is expected to  be a set of links\n",
    "     # Extract tifs using regular expressions\n",
    "     links = re.findall(r'(https?://[^\\s]+\\.(?:tif|tiff))', response.text) # s?: 's' character optional due to the ? quantifier. / [^\\s]: Any character except whitespace (\\s) / + quantifier ensures that there is at least one non-whitespace character.\n",
    "     for tif_url in links:\n",
    "          filename = os.path.basename(tif_url)\n",
    "          file_location = os.path.join(download_folder, filename) # location of the download\n",
    "          web_file_list.append(filename)\n",
    "          tif_response = requests.get(tif_url)\n",
    "          print(\"Downloading: {}\".format(filename))\n",
    "          download_file(tif_url, file_location)\n",
    "else:\n",
    "     # Download each treecover*.tif file into the created folder\n",
    "    for link in links:\n",
    "        href = link.get('href')  # the name of the file \"kjdfnfosdjf.tif\"\n",
    "        if href and href.endswith('.tif') and href.startswith('treecover'):\n",
    "            file_location = os.path.join(download_folder, href) # location of the download\n",
    "            web_file_list.append(href)\n",
    "            file_url = url + href  # We build the sentence\n",
    "            print(\"Downloading: \", file_location)\n",
    "            download_file(file_url, file_location)\n",
    "\n",
    "print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Check for missing files\"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m web_file_list \u001b[38;5;241m=\u001b[39m \u001b[43mmake_raster_link_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# List all files in the download folder\u001b[39;00m\n\u001b[0;32m      4\u001b[0m local_file_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(download_folder)\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36mmake_raster_link_list\u001b[1;34m(links)\u001b[0m\n\u001b[0;32m     12\u001b[0m web_file_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[1;32m---> 14\u001b[0m     href \u001b[38;5;241m=\u001b[39m \u001b[43mlink\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# the name of the file\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreecover\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     16\u001b[0m         web_file_list\u001b[38;5;241m.\u001b[39mappend(href)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "\"\"\"Check for missing files\"\"\"\n",
    "# This works only if we use \"href\" / need to develop this further\n",
    "# web_file_list = make_raster_link_list(links)\n",
    "\n",
    "web_file_list = links\n",
    "# List all files in the download folder\n",
    "local_file_list = os.listdir(download_folder)\n",
    "\n",
    "# Convert both lists to sets for efficient comparison. Set eveything to minus, the files are different in naming.\n",
    "downloaded_files_set = set(map(str.lower, local_file_list))\n",
    "website_files_set = set(map(str.lower, web_file_list))\n",
    "\n",
    "# Find missing files by taking the difference between the sets\n",
    "missing_files = website_files_set - downloaded_files_set\n",
    "\n",
    "if missing_files:\n",
    "    print(\"The following files are missing:\")\n",
    "    for file in missing_files:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files are missing:\n",
      "treecover2010_20n_120e.tif\n",
      "treecover2010_20s_010e.tif\n",
      "treecover2010_20n_110w.tif\n",
      "treecover2010_20n_110e.tif\n",
      "treecover2010_20n_160w.tif\n",
      "Missing files downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Download missing files\"\"\"\n",
    "for missing_file in missing_files:\n",
    "    layer_count = 1\n",
    "    file_url = url + missing_file\n",
    "    file_name = os.path.join(download_folder, missing_file)\n",
    "    print(\"Downloading: \", file_name, \"{} out of {}\".format(layer_count, len(missing_files)))\n",
    "    download_file(file_url, file_name)\n",
    "    layer_count += 1\n",
    "    \n",
    "print(\"Missing files downloaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_forge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
