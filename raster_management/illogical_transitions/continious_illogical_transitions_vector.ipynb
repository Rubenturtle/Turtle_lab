{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import rasterio\n",
    "# from rasterio.features import shapes\n",
    "from rasterio import features\n",
    "from rasterio.transform import from_origin\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from osgeo import gdal, ogr, osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_if_not_exists(folder_path):\n",
    "    \"\"\"\n",
    "    Create a folder if it doesn't exist.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path of the folder to be created.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder created at: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Folder already exists at: {folder_path}\")\n",
    "\n",
    "def get_vector_file_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of the vector files inside the folder\n",
    "    Parameters:\n",
    "    - path (str): path of the folder with the resources.\n",
    "\n",
    "    Returns:\n",
    "    - File_list (list). list of the resources.\n",
    "    \"\"\"\n",
    "    File_list = [] #f for f in os.listdir(path) if os.isfile(mypath,f)\n",
    "    for file in os.listdir(path):\n",
    "        # \"anat\" is just to get here necessary ones\n",
    "        if file.endswith(\".shp\"):\n",
    "            if file not in File_list:\n",
    "                File_list.append(os.path.join(path,file))\n",
    "        else:\n",
    "            pass\n",
    "    return File_list\n",
    "\n",
    "def get_raster_file_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of the raster files inside the folder\n",
    "    Parameters:\n",
    "    - path (str): path of the folder with the resources.\n",
    "\n",
    "    Returns:\n",
    "    - File_list (list). list of the resources.\n",
    "    \"\"\"\n",
    "    File_list = [] #f for f in os.listdir(path) if os.isfile(mypath,f)\n",
    "    for file in os.listdir(path):\n",
    "        # \"32628\" is just to get here necessary ones\n",
    "        if file.endswith(\".tif\") or file.endswith(\".tiff\"):\n",
    "            if file not in File_list:\n",
    "                File_list.append(os.path.join(path,file))\n",
    "        else:\n",
    "            pass\n",
    "    return File_list\n",
    "\n",
    "def get_csv_file_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of the csv files inside the folder\n",
    "    Parameters:\n",
    "    - path (str): path of the folder with the resources.\n",
    "\n",
    "    Returns:\n",
    "    - File_list (list). list of the resources.\n",
    "    \"\"\"\n",
    "    File_list = [] #f for f in os.listdir(path) if os.isfile(mypath,f)\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            if file not in File_list:\n",
    "                File_list.append(os.path.join(path,file))\n",
    "        else:\n",
    "            pass\n",
    "    return File_list\n",
    "\n",
    "def update_names_based_on_similarity(unique_names, gdf, column_name, similarity_threshold=0):\n",
    "    \"\"\"\n",
    "    Update names in gdf based on similarity to names in unique list.\n",
    "\n",
    "    Parameters:\n",
    "    - unique_names (list): list of the unique names.\n",
    "    - gdf (GeoDataFrame): GeoDataFrame whose names need to be updated.\n",
    "    - column_name (str): String of the column.\n",
    "    - similarity_threshold (float): Threshold for similarity ratio.\n",
    "\n",
    "    Returns:\n",
    "    - gdf. Updates gdf in place.\n",
    "    \"\"\"\n",
    "    # Add a new column 'valid_text' with None values\n",
    "    gdf['valid_text'] = None\n",
    "\n",
    "    total_elements = len(gdf)  # Get total number of elements\n",
    "\n",
    "    # Iterate through rows of gdf2\n",
    "    for index, row in gdf.iterrows():\n",
    "\n",
    "        # Get the value of the column for the current row\n",
    "        name_gdf = row[column_name]\n",
    "        highest_similarity_ratio = 0\n",
    "        best_matching_name = None\n",
    "        # Iterate through unique names in gdf1\n",
    "        for unique_name in unique_names:\n",
    "            # Calculate similarity ratio between names in gdf2 and gdf1\n",
    "            similarity_ratio = SequenceMatcher(None, unique_name, name_gdf).ratio()\n",
    "            # Update best matching name if similarity ratio is higher\n",
    "            if similarity_ratio > highest_similarity_ratio:\n",
    "                highest_similarity_ratio = similarity_ratio\n",
    "                best_matching_name = unique_name\n",
    "\n",
    "        if highest_similarity_ratio >= similarity_threshold:\n",
    "            gdf.at[index, 'valid_text'] = best_matching_name\n",
    "            gdf.at[index, 'simil_val'] = highest_similarity_ratio\n",
    "\n",
    "        print(f\"Processing element {index + 1}/{total_elements}\", end=\"\\r\") # This is to track the process\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def create_identifier_dictionary(list):\n",
    "    \"\"\"\n",
    "    Creates a dictionary out of a list appending a new id to each one of them.\n",
    "\n",
    "    Parameters:\n",
    "    - list (list): list of strings.\n",
    "\n",
    "    Returns:\n",
    "    - value_to_text_dict. dictionary o value: text.\n",
    "    \"\"\"\n",
    "    value_to_text_dict = {value: index + 1 for index, value in enumerate(sorted(list))}\n",
    "    return value_to_text_dict\n",
    "\n",
    "def rasterize_geodataframe_by_column(gdf, value_to_index, resolution, nodata_value, data_type, output_path, cols, rows):\n",
    "    \"\"\"\n",
    "    Rasterizes a geodataframe based on the column field.\n",
    "\n",
    "    Parameters:\n",
    "    - gdf (GeoDataFrame): GeoDataFrame to be rasterized.\n",
    "    - column_name (str): String of the column name.\n",
    "    - resolution (int): resolution of the raster.\n",
    "    - output_path (str): output of the raster file.\n",
    "\n",
    "    Returns:\n",
    "    - None. Rasterizes the geodataframe.\n",
    "    \"\"\"\n",
    "       \n",
    "    # Get the bounds of the GeoDataFrame\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "  \n",
    "    # Create a transform for the raster\n",
    "    transform = from_origin(xmin, ymax, resolution, resolution)\n",
    "\n",
    "    # Create an empty array to hold the rasterized values\n",
    "    rasterized_array = np.full((rows, cols), nodata_value, dtype=data_type)\n",
    "\n",
    "    total_values = len(value_to_index)\n",
    "\n",
    "    # Rasterize each unique value separately\n",
    "    for idx, (text_value, value) in enumerate(value_to_index.items()):\n",
    "        print(f\"Processing {idx + 1} out of {total_values}\")\n",
    "        mask = gdf['raster_val'] == value\n",
    "        shapes = gdf.loc[mask, 'geometry']\n",
    "        if shapes.empty:\n",
    "            # The value_to_index contains all the possible parameters, but there are some that don't exists in a certain file\n",
    "            continue\n",
    "\n",
    "        temp_raster = features.rasterize(\n",
    "            shapes=shapes,\n",
    "            out_shape=(rows, cols),\n",
    "            transform=transform,\n",
    "            all_touched=True, # Esto asegura que si toca la linea del poligono, se genera el pixel\n",
    "            default_value=value,\n",
    "            dtype=data_type, # must be equal to the zeros # np.uint8\n",
    "        )\n",
    "        # Combine the current raster with the cumulative raster using maximum value\n",
    "        rasterized_array = np.where(temp_raster != 0, temp_raster, rasterized_array)\n",
    "\n",
    "    crs = gdf.crs\n",
    "\n",
    "    # Define the metadata for the raster\n",
    "    profile = {\n",
    "        'driver': 'GTiff',\n",
    "        'height': rows,\n",
    "        'width': cols,\n",
    "        'count': 1,\n",
    "        'dtype': data_type,\n",
    "        'crs': crs, #CRS.from_epsg(32628),\n",
    "        'transform': transform,\n",
    "        'nodata': nodata_value,  # Set the nodata value in the profile metadata\n",
    "        'compress': 'deflate',  # Compression method\n",
    "        'tiled': True,  # Enable tiling\n",
    "        # 'legend': {str(key): value for key, value in value_to_index.items()}\n",
    "    }\n",
    "\n",
    "    # Write the raster array to a GeoTIFF file\n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        dst.write(rasterized_array, 1)\n",
    "\n",
    "def gdal_rasterize_from_shapefile(shapefile_path, resolution, nodata_value, data_type, output_path, cols=None, rows=None):\n",
    "    \"\"\"\n",
    "    Rasterizes a GeoDataFrame using GDAL directly.\n",
    "\n",
    "    Parameters:\n",
    "    - shapefile_path (string): path of the vector file.\n",
    "    - resolution (int or float): Resolution of the raster (pixel size).\n",
    "    - nodata_value: The value to use for no-data pixels.\n",
    "    - data_type: Data type for the output raster (e.g., gdal.GDT_Float32).\n",
    "    - output_path (str): Path to save the output raster.\n",
    "    - cols (int, optional): Number of columns in the output raster.\n",
    "    - rows (int, optional): Number of rows in the output raster.\n",
    "\n",
    "    Returns:\n",
    "    - None. The function writes the raster to the specified output path.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the Shapefile using OGR\n",
    "    shapefile = ogr.Open(shapefile_path)\n",
    "    layer = shapefile.GetLayer()\n",
    "\n",
    "    # Get the bounds of the Shapefile (same as GeoDataFrame's total_bounds)\n",
    "    xmin, xmax, ymin, ymax = layer.GetExtent()\n",
    "\n",
    "    # If cols and rows are not provided, calculate them based on resolution\n",
    "    if cols is None or rows is None:\n",
    "        cols = int((xmax - xmin) / resolution)\n",
    "        rows = int((ymax - ymin) / resolution)\n",
    "\n",
    "    # Create a new raster dataset\n",
    "    raster_ds = gdal.GetDriverByName('GTiff').Create(\n",
    "        output_path, cols, rows, 1, data_type,\n",
    "        options=['COMPRESS=DEFLATE', 'TILED=YES']\n",
    "    )\n",
    "\n",
    "    # Set the geotransform (affine transform for the raster)\n",
    "    geotransform = (xmin, resolution, 0, ymax, 0, -resolution)\n",
    "    raster_ds.SetGeoTransform(geotransform)\n",
    "\n",
    "    # Set the CRS (coordinate reference system) from the Shapefile\n",
    "    srs = layer.GetSpatialRef()\n",
    "    if srs:\n",
    "        raster_ds.SetProjection(srs.ExportToWkt())\n",
    "\n",
    "    # Create the raster band and set no-data value\n",
    "    band = raster_ds.GetRasterBand(1)\n",
    "    band.SetNoDataValue(nodata_value)\n",
    "\n",
    "    # Rasterize the shapefile\n",
    "    gdal.RasterizeLayer(\n",
    "        raster_ds,  # Output raster dataset\n",
    "        [1],        # Raster band to write to\n",
    "        layer,      # Input OGR layer to rasterize\n",
    "        options=['ATTRIBUTE=raster_val', 'ALL_TOUCHED=TRUE']\n",
    "    )\n",
    "\n",
    "    # Flush and close the raster dataset\n",
    "    band.FlushCache()\n",
    "    raster_ds = None  # Close the file and save\n",
    "\n",
    "    # Close the shapefile\n",
    "    shapefile = None\n",
    "\n",
    "    print(f\"Rasterization complete: {output_path}\")\n",
    "\n",
    "def check_same_dimensions(raster_files):\n",
    "    \"\"\"\n",
    "    Check the if the dimensions of all the input rasters have the same dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_files (list): List of raster files.\n",
    "\n",
    "    Returns:\n",
    "    - dimensions_list (list). The list with all the dimensions.\n",
    "    \"\"\"\n",
    "    dimensions_list = []\n",
    "\n",
    "    # Open the first raster file in the list\n",
    "    for file_path in raster_files[:]:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            first_shape = src.shape\n",
    "            shape_dimensions = src.shape\n",
    "            dimensions_list.append(shape_dimensions)\n",
    "\n",
    "    if len(set(dimensions_list)) == 1:\n",
    "        print(f\"All the elements have the same dimensions{dimensions_list[0]}\")\n",
    "        dimensions_list\n",
    "    else:\n",
    "        print(\"The dimensions are note the same\")\n",
    "        return dimensions_list\n",
    "\n",
    "\n",
    "def read_csv_in_pairs(csv_file):\n",
    "    \"\"\"\n",
    "    Reads a two column csv and transforms it into a pair value list.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file (str): path of the csv file.\n",
    "\n",
    "    Returns:\n",
    "    - rule_values_list: list of unique value pairs.\n",
    "    \"\"\"\n",
    "    rule_values_list = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        # next(file)  # Skip the header row\n",
    "        for line in file:\n",
    "            # Split the line into two values based on spaces, and convert them to floats\n",
    "            rule_value_1, rule_value_2 = map(float, line.split(\",\"))\n",
    "            rule_values_list.append((rule_value_1, rule_value_2))\n",
    "    return rule_values_list\n",
    "\n",
    "def read_csv_in_trios(csv_file):\n",
    "    \"\"\"\n",
    "    Reads a three-column csv and transforms it into a dictionary of id and pair value (value1, value2).\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file (str): path of the csv file.\n",
    "\n",
    "    Returns:\n",
    "    - rule_values_dict: dictionary where keys are IDs and values are unique value pairs (value1, value2).\n",
    "    \"\"\"\n",
    "    rule_values_dict = {}\n",
    "    with open(csv_file, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line into three values: ID, value1, and value2, then convert value1 and value2 to floats\n",
    "            id, rule_value_1, rule_value_2 = line.split(\",\")\n",
    "            rule_values_dict[id] = (float(rule_value_1), float(rule_value_2))\n",
    "    \n",
    "    return rule_values_dict\n",
    "\n",
    "def read_csv_columns_and_data(csv_file):\n",
    "    \"\"\"\n",
    "    Reads a two column csv and transforms it into a pair value list.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file (str): path of the csv file.\n",
    "\n",
    "    Returns:\n",
    "    - columns: list of columns.\n",
    "    - rows: list of rows.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = ()\n",
    "    with open(csv_file, 'r') as file:\n",
    "        for line in file:\n",
    "            if len(columns) == 0:\n",
    "                columns = (line)\n",
    "            values = line.strip().split(\",\")\n",
    "            rows.append(values)\n",
    "    return columns, rows\n",
    "\n",
    "def csv_to_dict(file_path):\n",
    "    \"\"\"\n",
    "    Creates a dictionary out of a csv of two columns excluding the header.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (string): lpath of the file.\n",
    "\n",
    "    Returns:\n",
    "    - result_dict(dict). dictionary o value: text.\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Open the CSV file\n",
    "    with open(file_path, mode='r', newline='', encoding='ISO-8859-1') as file: # encoding='ISO-8859-1' encoding='utf-8'\n",
    "        reader = csv.reader(file)\n",
    "        \n",
    "        # Skip the header\n",
    "        next(reader)\n",
    "        \n",
    "        # Iterate through each row and add to the dictionary\n",
    "        for row in reader:\n",
    "            key = row[1]  # First column as the key\n",
    "            value = row[0]  # Second column as the value\n",
    "            result_dict[float(key)] = value\n",
    "            \n",
    "    return result_dict\n",
    "\n",
    "def generate_random_pairs(paths_list):\n",
    "    \"\"\"\n",
    "    Generates a list of random unique pairs, from the input list.\n",
    "\n",
    "    Parameters:\n",
    "    - paths_list (list): list of paths.\n",
    "\n",
    "    Returns:\n",
    "    - all_pairs: list of unique value pairs.\n",
    "    \"\"\"\n",
    "    # Generate all possible pairs from the raster paths\n",
    "    all_pairs = list(itertools.combinations(paths_list, 2))\n",
    "    # Randomly shuffle the list of pairs\n",
    "    random.shuffle(all_pairs)\n",
    "    return all_pairs\n",
    "\n",
    "\n",
    "def compare_rasters(raster_pair, output, rule_table_path):\n",
    "    \"\"\"\n",
    "    Generates a list of random unique pairs, from the input list.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_pair (list): list of two elements.\n",
    "    - output (path): path of the output file.\n",
    "    - rule_table_path (list): is a list of pairs.\n",
    "\n",
    "    Returns:\n",
    "    - output_loc (str): path out the ourput file. \n",
    "    - comparison_df: dataframe related to the data.\n",
    "    \"\"\"\n",
    "    # Open raster files\n",
    "    ds1 = gdal.Open(raster_pair[0])\n",
    "    ds2 = gdal.Open(raster_pair[1])\n",
    "\n",
    "    if not ds1 or not ds2:\n",
    "        print(\"Error: Unable to open raster files.\")\n",
    "        return\n",
    "    \n",
    "    # Check if both rasters have the same height and width\n",
    "    if ds1.RasterXSize != ds2.RasterXSize or ds1.RasterYSize != ds2.RasterYSize:\n",
    "        print(\"Error: Rasters have different dimensions.\")\n",
    "    \n",
    "    # Get the first raster information\n",
    "    width = ds1.RasterXSize\n",
    "    height = ds1.RasterYSize\n",
    "    geotransform = ds1.GetGeoTransform()\n",
    "    projection = ds1.GetProjection()\n",
    "    \n",
    "    # Read rule table and create a list of pairs with the info\n",
    "    # rule_values_list = read_csv_in_pairs(rule_table_path)\n",
    "    rule_values_dict = read_csv_in_trios(rule_table_path)\n",
    "\n",
    "    # Create output raster\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    year1 = os.path.basename(raster_pair[0]).split('_')[-1].replace(\".tif\",\"\") # Get always the last element\n",
    "    year2 = os.path.basename(raster_pair[1]).split('_')[-1].replace(\".tif\",\"\")\n",
    "    output_filename = f\"{year1}_{year2}_illogical_transitions.tif\" #Customize\n",
    "    output_loc = os.path.join(output, output_filename)\n",
    "\n",
    "    output_ds = driver.Create(output_loc, width, height, 1, gdal.GDT_Int16, options= ['COMPRESS=DEFLATE', 'TILED=YES']) # GDT_Int32\n",
    "    output_ds.GetRasterBand(1).SetNoDataValue(0)\n",
    "    output_ds.SetGeoTransform(geotransform)\n",
    "    output_ds.SetProjection(projection)\n",
    "\n",
    "    output_array = np.zeros((height, width), dtype=np.int16) # int16\n",
    "\n",
    "    unique_value_dict = {} # pairs : unique_value\n",
    "\n",
    "\n",
    "    # Loop through each pixel and compare values\n",
    "    block_size = 256  # Adjust the block size as needed\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            print(f\"Comparing pixels at rows/columns ({x},{y}) from ({width}, {height})\", end='\\r')\n",
    "            block_width = min(block_size, width - x)\n",
    "            block_height = min(block_size, height - y)\n",
    "\n",
    "            block1 = ds1.GetRasterBand(1).ReadAsArray(x, y, block_width, block_height)\n",
    "            block2 = ds2.GetRasterBand(1).ReadAsArray(x, y, block_width, block_height)\n",
    "    \n",
    "            # Check if the pair of values matches any rule pair\n",
    "            correct_value = 0\n",
    "            \n",
    "            # rule_values_list = set(rule_values_list) # To speed up the process, don't use list\n",
    "            for i in range(block_height):\n",
    "                for j in range(block_width):\n",
    "                    value1 = block1[i, j]\n",
    "                    value2 = block2[i, j]\n",
    "\n",
    "                    if (value1, value2) in rule_values_dict:\n",
    "                        # unique_value = unique_value_dict.setdefault((value1, value2), len(unique_value_dict) + 1)\n",
    "                        unique_value = rule_values_dict[(value1, value2)]\n",
    "                        output_array[y + i, x + j] = unique_value\n",
    "\n",
    "                    else:\n",
    "                        output_array[y + i, x + j] = correct_value\n",
    "\n",
    "    # Write the output\n",
    "    output_ds.GetRasterBand(1).WriteArray(output_array)\n",
    "\n",
    "    # Close datasets\n",
    "    ds1 = None\n",
    "    ds2 = None\n",
    "    output_ds = None\n",
    "\n",
    "    # Create the dataframe\n",
    "    unique_values = list(unique_value_dict.values())\n",
    "    value_pairs = list(unique_value_dict.keys())\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'univalue': unique_values,\n",
    "        f'Val1_{year1}': [pair[0] for pair in value_pairs],\n",
    "        f'Val2_{year2}': [pair[1] for pair in value_pairs]\n",
    "                                })\n",
    "    \n",
    "    # Lets export the csv just in case\n",
    "    comparison_df.to_csv(os.path.join(output, f\"illogical_table_{year1}_{year2}.csv\"), index=False)\n",
    "\n",
    "    return output_loc, comparison_df\n",
    "\n",
    "\n",
    "def vectorize_raster(raster_path, output_path, df):\n",
    "    \"\"\"\n",
    "    Vectorizes an input raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): path of the input raster.\n",
    "    - output_path (path): path of the output file.\n",
    "    - df (dataframe): dataframe related to the input raster.\n",
    "\n",
    "    Returns:\n",
    "    - None. It produces the vector file. \n",
    "    \"\"\"\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Read raster data as numpy array\n",
    "        data = src.read(1)\n",
    "        # Get affine transform of the raster\n",
    "        transform = src.transform\n",
    "\n",
    "    # Vectorize the raster data\n",
    "    raster_shapes = features.shapes(data, transform=transform)\n",
    "\n",
    "    # Convert the vectorized shapes into a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame.from_features(\n",
    "        [\n",
    "            {\"geometry\": geo_shape, \"properties\": {\"univalue\": value}}\n",
    "            for geo_shape, value in raster_shapes\n",
    "        ],\n",
    "        crs=src.crs\n",
    "    )\n",
    "\n",
    "    # Merge the dataframe with the GeoDataFrame\n",
    "    merged_gdf = gdf.merge(df, on='univalue')\n",
    "    # Dissolve based on a column value\n",
    "    dissolved_gdf = merged_gdf.dissolve(by='univalue')\n",
    "\n",
    "    # Save the merged GeoDataFrame as a new shapefile\n",
    "    filename = os.path.join(output_path, os.path.basename(raster_path).split('.')[0])\n",
    "    dissolved_gdf.to_file(f'{filename}.shp', driver='ESRI Shapefile')\n",
    "    return\n",
    "\n",
    "def add_name_columns_to_dataframe(df, column_name, names_dictionary):\n",
    "    \"\"\"\n",
    "    Adds the input name list into the  an input raster.\n",
    "\n",
    "    Parameters:\n",
    "    - df (dataframe): dataframe related to the input raster.\n",
    "    - column_name (str): name of the column that is appeended to the df.\n",
    "    - names_dictionary (dict): dict of the names that we want to append.\n",
    "\n",
    "    Returns:\n",
    "    - df (dataframe). It produces the updated dataframe with the names. \n",
    "    \"\"\"\n",
    "    reversed_dict = {v: k for k, v in names_dictionary.items()} # Reverse it to match the values / value: text\n",
    "    count = 1\n",
    "    for column in df.columns:\n",
    "        if column.startswith(f'Val{count}'):\n",
    "            #Creates a new column witht the column_name matching the value of the value.\n",
    "            df[column_name + str(count)] = df[column].map(reversed_dict)\n",
    "            count += 1\n",
    "    return df\n",
    "\n",
    "def consecutive_pairs(list):\n",
    "    \"\"\"\n",
    "    Creates a consecutive pairs list.\n",
    "\n",
    "    Parameters:\n",
    "    - list (list): list of files.\n",
    "\n",
    "    Returns:\n",
    "    - pairs (list): list of pairs in order. \n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i in range(len(list) - 1):\n",
    "        pairs.append((list[i], list[i + 1]))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def dissolve_geodataframe(gdf, column):\n",
    "    \"\"\"\n",
    "    Dissolves a GeoDataFrame based on unique values of a specified column.\n",
    "\n",
    "    Parameters:\n",
    "    gdf (GeoDataFrame): Input GeoDataFrame to be dissolved.\n",
    "    column (str): Column name based on which to dissolve the GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: The dissolved GeoDataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Initial number of geometries: {len(gdf)}\")\n",
    "    \n",
    "    # Perform the dissolve operation\n",
    "    dissolved_gdf = gdf.dissolve(by=column)\n",
    "    \n",
    "    print(f\"Final number of geometries after dissolve: {len(dissolved_gdf)}\")\n",
    "    \n",
    "    return dissolved_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: Y:\\z_resources\\ruben\\landcover_vector_files_copy\\output_files\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Specify all the inputs\"\"\"\n",
    "path = r\"Y:\\z_resources\\ruben\\landcover_vector_files_copy\" #\"Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\"\n",
    "rule_table_path = \"Z:\\data\\im-nca-colombia\\landcover\\landcover_vector_files_copy\\illogical_transitions\\output_files\\illogical_transitions.csv\" # \"Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\illogical_transitions.csv\"\"\n",
    "output_path = path + r\"\\output_files\"\n",
    "create_folder_if_not_exists(output_path)\n",
    "\n",
    "\"\"\"We have here two situation we have to develop\"\"\"\n",
    "\n",
    "# We only have a list of valid strings\n",
    "names_list =  [\"Territorios artificializados\", \"Zonas urbanizadas\", \"Tejido urbano continuo\", \"Tejido urbano discontinuo\", \"Zonas industriales o comerciales y redes de comunicación\", \"Zonas industriales o comerciales\",     \"Zonas industriales\", \"Zonas comerciales\", \"Red vial, ferroviaria y terrenos asociados\",     \"Red vial y territorios asociados\", \"Red ferroviaria y terrenos asociados\", \"Zonas portuarias\",     \"Zonas portuarias fluviales\", \"Zonas portuarias marítimas\", \"Aeropuertos\", \"Aeropuerto con infraestructura asociada\",     \"Aeropuerto sin infraestructura asociada\", \"Obras hidráulicas\", \"Zonas de extracción minera y escombreras\",     \"Zonas de extracción minera\", \"Otras explotaciones mineras\", \"Explotación de hidrocarburos\",     \"Explotación de carbón\", \"Explotación de oro\", \"Explotación de materiales de construcción\",     \"Explotación de sal\", \"Otros sitios de disposición de residuos a cielo abierto\", \"Escombreras\",     \"Vertederos\", \"Relleno sanitario\", \"Zonas verdes artificializadas, no agrícolas\", \"Zonas verdes urbanas\",     \"Otras zonas verdes urbanas\", \"Parques cementerios\", \"Jardines botánicos\", \"Zoológicos\", \"Parques urbanos\",     \"Rondas de cuerpos de agua de zonas urbanas\", \"Instalaciones recreativas\", \"Áreas culturales\",     \"Áreas deportivas\", \"Áreas turísticas\", \"Territorios agrícolas\", \"Cultivos transitorios\",     \"Otros cultivos transitorios\", \"Cereales\", \"Arroz\", \"Maíz\", \"Sorgo\", \"Cebada\", \"Trigo\",     \"Oleaginosas y leguminosas\", \"Algodón\", \"Ajonjolí\", \"Frijol\", \"Soya\", \"Maní\", \"Hortalizas\",     \"Cebolla\", \"Zanahoria\", \"Remolacha\", \"Tubérculos\", \"Papa\", \"Yuca\", \"Cultivos permanentes\",     \"Cultivos permanentes herbáceos\", \"Otros cultivos permanentes herbáceos\", \"Caña\", \"Caña de azúcar\",     \"Caña panelera\", \"Plátano y banano\", \"Tabaco\", \"Papaya\", \"Amapola\", \"Cultivos permanentes arbustivos\",     \"Otros cultivos permanentes arbustivos\", \"Café\", \"Cacao\", \"Viñedos\", \"Coca\", \"Cultivos permanentes arbóreos\",     \"Otros cultivos permanentes arbóreos\", \"Palma de aceite\", \"Cítricos\", \"Mango\", \"Cultivos agroforestales\",     \"Pastos y árboles plantados\", \"Cultivos y árboles plantados\", \"Cultivos confinados\", \"Pastos\",     \"Pastos limpios\", \"Pastos arbolados\", \"Pastos enmalezados\", \"Áreas agrícolas heterogéneas\",     \"Mosaico de cultivos\", \"Mosaico de pastos y cultivos\", \"Mosaico de cultivos, pastos y espacios naturales\",     \"Mosaico de pastos y espacios naturales\", \"Mosaico de cultivos con espacios naturales\", \"Bosques y áreas seminaturales\",     \"Bosques\", \"Bosque denso\", \"Bosque denso alto\", \"Bosque denso alto de tierra firme\", \"Bosque denso alto inundable\",     \"Bosque denso alto inundable heterogéneo\", \"Manglar denso alto\", \"Manglar de aguas marinas\",     \"Manglar de aguas mixohalinas\", \"Palmares\", \"Bosque denso bajo\", \"Bosque denso bajo de tierra firme\",     \"Bosque denso bajo inundable\", \"Bosque abierto\", \"Bosque abierto alto\", \"Bosque abierto alto de tierra firme\",     \"Bosque abierto alto inundable\", \"Bosque abierto bajo\", \"Bosque abierto bajo de tierra firme\",     \"Bosque abierto bajo inundable\", \"Bosque fragmentado\", \"Bosque fragmentado con pastos y cultivos\",     \"Bosque fragmentado con vegetación secundaria\", \"Bosque de galería y ripario\", \"Plantación forestal\",     \"Plantación de coníferas\", \"Plantación de latifoliadas\", \"Bosque mixto de guandal\", \"Áreas con vegetación herbácea y/o arbustiva\",     \"Herbazal\", \"Herbazal denso\", \"Herbazal denso de tierra firme\", \"Herbazal denso de tierra firme no arbolado\",     \"Herbazal denso de tierra firme arbolado\", \"Herbazal denso de tierra firme con arbustos\", \"Herbazal denso inundable\",     \"Herbazal denso inundable no arbolado\", \"Herbazal denso inundable arbolado\", \"Arracachal\", \"Helecho\",     \"Herbazal abierto\", \"Herbazal abierto arenoso\", \"Herbazal abierto rocoso\", \"Arbustal\", \"Arbustal denso\",     \"Arbustal abierto\", \"Arbustal abierto esclerófilo\", \"Arbustal abierto mesófilo\", \"Vegetación secundaria en transición\",     \"Vegetación secundaria alta\", \"Vegetación secundaria baja\", \"Áreas abiertas, con o sin vegetación esparcida\",     \"Zonas arenosas naturales\", \"Playas\", \"Arenales\", \"Campos de dunas\", \"Afloramientos rocosos\", \"Tierras desnudas y degradadas\",     \"Zonas quemadas\", \"Zonas glaciales y nivales\", \"Zonas glaciares\", \"Zonas nivales\", \"Áreas Húmedas\",     \"Áreas húmedas continentales\", \"Zonas Pantanosas\", \"Turberas\", \"Vegetación acuática sobre cuerpos de agua\",     \"Áreas húmedas costeras\", \"Pantanos costeros\", \"Salitral\", \"Sedimentos expuestos en bajamar\", \"Superficies de Agua\",     \"Aguas continentales\", \"Ríos\", \"Lagunas, lagos y ciénagas naturales\", \"Laguna\", \"Lago\", \"Canales\",     \"Cuerpos de agua artificiales\", \"Embalses\", \"Lagunas de oxidación\", \"Estanques para acuicultura continental\",     \"Aguas marítimas\", \"Lagunas costeras\", \"Mares y océanos\", \"Otros fondos\", \"Fondos coralinos someros\",     \"Praderas de pastos marinos someros\", \"Fondos someros de arenas y cascajo\", \"Estanques para acuicultura marina\"]\n",
    "# Here we set a value to each string, since in this case it does not have it\n",
    "# names_dictionary = create_identifier_dictionary(names_list) # The input will always be text + id\n",
    "\n",
    "# Here we have a dictionary\n",
    "names_dictionary_path = r\"Y:\\z_resources\\ruben\\landcover_vector_files_copy\\names_dictionary.csv\"\n",
    "names_dictionary = csv_to_dict(names_dictionary_path)\n",
    "# names_dictionary = {\"Territorios artificializados\": 1, \"Zonas urbanizadas\": 11, \"Tejido urbano continuo\": 111, \"Tejido urbano discontinuo\": 112, \"Zonas industriales o comerciales y redes de comunicación\": 12, \"Zonas industriales o comerciales\": 121, \"Zonas industriales\": 1211, \"Zonas comerciales\": 1212, \"Red vial, ferroviaria y terrenos asociados\": 122, \"Red vial y territorios asociados\": 1221, \"Red ferroviaria y terrenos asociados\": 1222, \"Zonas portuarias\": 123, \"Zonas portuarias fluviales\": 1231, \"Zonas portuarias marítimas\": 1232, \"Aeropuertos\": 124, \"Aeropuerto con infraestructura asociada\": 1241, \"Aeropuerto sin infraestructura asociada\": 1242, \"Obras hidráulicas\": 125, \"Zonas de extracción minera y escombreras\": 13, \"Zonas de extracción minera\": 131, \"Otras explotaciones mineras\": 1311, \"Explotación de hidrocarburos\": 1312, \"Explotación de carbón\": 1313, \"Explotación de oro\": 1314, \"Explotación de materiales de construcción\": 1315, \"Explotación de sal\": 1316, \"Otros sitios de disposición de residuos a cielo abierto\": 132, \"Escombreras\": 1321, \"Vertederos\": 1322, \"Relleno sanitario\": 1323, \"Zonas verdes artificializadas, no agrícolas\": 14, \"Zonas verdes urbanas\": 141, \"Otras zonas verdes urbanas\": 1411, \"Parques cementerios\": 1412, \"Jardines botánicos\": 1413, \"Zoológicos\": 1414, \"Parques urbanos\": 1415, \"Rondas de cuerpos de agua de zonas urbanas\": 1416, \"Instalaciones recreativas\": 142, \"Áreas culturales\": 1421, \"Áreas deportivas\": 1422, \"Áreas turísticas\": 1423, \"Territorios agrícolas\": 2, \"Cultivos transitorios\": 21, \"Otros cultivos transitorios\": 211, \"Cereales\": 212, \"Arroz\": 2121, \"Maíz\": 2122, \"Sorgo\": 2123, \"Cebada\": 2124, \"Trigo\": 2125, \"Oleaginosas y leguminosas\": 213, \"Algodón\": 2131, \"Ajonjolí\": 2132, \"Frijol\": 2133, \"Soya\": 2134, \"Maní\": 2135, \"Hortalizas\": 214, \"Cebolla\": 2141, \"Zanahoria\": 2142, \"Remolacha\": 2143, \"Tubérculos\": 215, \"Papa\": 2151, \"Yuca\": 2152, \"Cultivos permanentes\": 22, \"Cultivos permanentes herbáceos\": 221, \"Otros cultivos permanentes herbáceos\": 2211, \"Caña\": 2212, \"Caña de azúcar\": 22121, \"Caña panelera\": 22122, \"Plátano y banano\": 2213, \"Tabaco\": 2214, \"Papaya\": 2215, \"Amapola\": 2216, \"Cultivos permanentes arbustivos\": 222, \"Otros cultivos permanentes arbustivos\": 2221, \"Café\": 2222, \"Cacao\": 2223, \"Viñedos\": 2224, \"Coca\": 2225, \"Cultivos permanentes arbóreos\": 223, \"Otros cultivos permanentes arbóreos\": 2231, \"Palma de aceite\": 2232, \"Cítricos\": 2233, \"Mango\": 2234, \"Cultivos agroforestales\": 224, \"Pastos y árboles plantados\": 2241, \"Cultivos y árboles plantados\": 2242, \"Cultivos confinados\": 225, \"Pastos\": 23, \"Pastos limpios\": 231, \"Pastos arbolados\": 232, \"Pastos enmalezados\": 233, \"Áreas agrícolas heterogéneas\": 24, \"Mosaico de cultivos\": 241, \"Mosaico de pastos y cultivos\": 242, \"Mosaico de cultivos, pastos y espacios naturales\": 243, \"Mosaico de pastos y espacios naturales\": 244, \"Mosaico de cultivos con espacios naturales\": 245, \"Bosques y áreas seminaturales\": 3, \"Bosques\": 31, \"Bosque denso\": 311, \"Bosque denso alto\": 3111, \"Bosque denso alto de tierra firme\": 31111, \"Bosque denso alto inundable\": 31112, \"Bosque denso alto inundable heterogéneo\": 311121, \"Manglar denso alto\": 311122, \"Manglar de aguas marinas\": 3111221, \"Manglar de aguas mixohalinas\": 3111222, \"Palmares\": 311123, \"Bosque denso bajo\": 3112, \"Bosque denso bajo de tierra firme\": 31121, \"Bosque denso bajo inundable\": 31122, \"Bosque abierto\": 312, \"Bosque abierto alto\": 3121, \"Bosque abierto alto de tierra firme\": 31211, \"Bosque abierto alto inundable\": 31212, \"Bosque abierto bajo\": 3122, \"Bosque abierto bajo de tierra firme\": 31221, \"Bosque abierto bajo inundable\": 31222, \"Bosque fragmentado\": 313, \"Bosque fragmentado con pastos y cultivos\": 3131, \"Bosque fragmentado con vegetación secundaria\": 3132, \"Bosque de galería y ripario\": 314, \"Plantación forestal\": 315, \"Plantación de coníferas\": 3151, \"Plantación de latifoliadas\": 3152, \"Bosque mixto de guandal\": 316, \"Áreas con vegetación herbácea y/o arbustiva\": 32, \"Herbazal\": 321, \"Herbazal denso\": 3211, \"Herbazal denso de tierra firme\": 32111, \"Herbazal denso de tierra firme no arbolado\": 321111, \"Herbazal denso de tierra firme arbolado\": 321112, \"Herbazal denso de tierra firme con arbustos\": 321113, \"Herbazal denso inundable\": 32112, \"Herbazal denso inundable no arbolado\": 321121, \"Herbazal denso inundable arbolado\": 321122, \"Arracachal\": 321123, \"Helecho\": 321124, \"Herbazal abierto\": 3212, \"Herbazal abierto arenoso\": 32121, \"Herbazal abierto rocoso\": 32122, \"Arbustal\": 322, \"Arbustal denso\": 3221, \"Arbustal abierto\": 3222, \"Arbustal abierto esclerófilo\": 32221, \"Arbustal abierto mesófilo\": 32222, \"Vegetación secundaria en transición\": 33, \"Vegetación secundaria alta\": 331, \"Vegetación secundaria baja\": 332, \"Áreas abiertas, con o sin vegetación esparcida\": 34, \"Zonas arenosas naturales\": 341, \"Playas\": 3411, \"Arenales\": 3412, \"Campos de dunas\": 3413, \"Afloramientos rocosos\": 342, \"Tierras desnudas y degradadas\": 35, \"Zonas quemadas\": 36, \"Zonas glaciales y nivales\": 37, \"Zonas glaciares\": 371, \"Zonas nivales\": 372, \"Áreas Húmedas\": 4, \"Áreas húmedas continentales\": 41, \"Zonas Pantanosas\": 411, \"Turberas\": 412, \"Vegetación acuática sobre cuerpos de agua\": 413, \"Áreas húmedas costeras\": 42, \"Pantanos costeros\": 421, \"Salitral\": 422, \"Sedimentos expuestos en bajamar\": 423, \"Superficies de Agua\": 5, \"Aguas continentales\": 51, \"Ríos\": 511, \"Lagunas, lagos y ciénagas naturales\": 512, \"Laguna\": 5121, \"Lago\": 5122, \"Canales\": 513, \"Cuerpos de agua artificiales\": 514, \"Embalses\": 5141, \"Lagunas de oxidación\": 5142, \"Estanques para acuicultura continental\": 5143, \"Aguas marítimas\": 52, \"Lagunas costeras\": 521, \"Mares y océanos\": 522, \"Otros fondos\": 523, \"Fondos coralinos someros\": 5222, \"Praderas de pastos marinos someros\": 5223, \"Fondos someros de arenas y cascajo\": 5224, \"Estanques para acuicultura marina\": 523}\n",
    "\n",
    "vector_file_list = get_vector_file_list(path)\n",
    "\n",
    "# Specify the column to rasterize by\n",
    "column_name = 'leyenda'\n",
    "# Define the resolution of your raster.\n",
    "resolution = 30  # in meters\n",
    "# Define the nodata value of your raster.\n",
    "nodata_value = 0\n",
    "# Define the data type of the raster.\n",
    "data_type = gdal.GDT_UInt16\n",
    "\"\"\"\n",
    "gdal.GDT_Byte,\n",
    "gdal.GDT_Int16,\n",
    "gdal.GDT_UInt16,\n",
    "gdal.GDT_Int32,\n",
    "gdal.GDT_UInt32,\n",
    "gdal.GDT_Float32,\n",
    "gdal.GDT_Float64\n",
    "\"\"\"\n",
    "# Define the rows and columns for the rasterization (All the files must have the same dimensions). You can take as reference any on the inputs\n",
    "rows = 54718\n",
    "columns = 65397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OPTIONAL: For heavy vector files,  we are going to dissolve based on the designed column, to simplify the rest of the process\"\"\"\n",
    "dissolved_files_path = path + r\"\\dissolved_files\"\n",
    "create_folder_if_not_exists(dissolved_files_path)\n",
    "\n",
    "for file in vector_file_list[:]:\n",
    "    gdf = gpd.read_file(file)\n",
    "    print(\"gdf opened\")\n",
    "    gdf = dissolve_geodataframe(gdf, column_name)\n",
    "    gdf.to_file(os.path.join(dissolved_files_path, os.path.basename(file).replace(\".shp\", \"_dissolved.shp\")) , driver='ESRI Shapefile')\n",
    "\n",
    "# We set the nex location of the files\n",
    "vector_file_list = get_vector_file_list(dissolved_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the \"raster_val\" according to the content of the columnn\"\"\"\n",
    "optimized_vector_files_path = path + r\"\\optimized_vector_files\"\n",
    "create_folder_if_not_exists(optimized_vector_files_path)\n",
    "\n",
    "for file in vector_file_list[:]:\n",
    "    gdf = gpd.read_file(file)\n",
    "    print(f\"gdf {os.path.basename(file)} opened\")\n",
    "    if gdf[column_name].dtype == object:\n",
    "        print(f\"The column '{column_name}' contains strings.\")\n",
    "        gdf = update_names_based_on_similarity(names_list, gdf, column_name, similarity_threshold=0.3)\n",
    "        print(\"names updated\")\n",
    "        # Add a new column to the GeoDataFrame containing the unique identifiers\n",
    "        gdf['raster_val'] = gdf[\"valid_text\"].map(names_dictionary)\n",
    "        \n",
    "    else:\n",
    "        print(f\"The column '{column_name}' does not contain strings.\")\n",
    "        # Get unique values/strings from the specified column, they are always sorted.\n",
    "        unique_values = sorted(gdf[column_name].unique())\n",
    "        # Create a dictionary one to one\n",
    "        value_to_index = {value: value for value in unique_values}\n",
    "        gdf['raster_val'] = gdf[column_name].map(value_to_index)\n",
    "    \n",
    "    # Opening and saving the file takes a lot of time\n",
    "    gdf.to_file(os.path.join(optimized_vector_files_path, os.path.basename(file).replace(\".shp\", \"_optimized.shp\")) , driver='ESRI Shapefile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:\\z_resources\\ruben\\landcover_vector_files_copy\\output_files\\cobertura_tierra_2020_epsg3316_100k_2020_dissolved_optimized.tif\n",
      "Rasterization complete: Y:\\z_resources\\ruben\\landcover_vector_files_copy\\output_files\\cobertura_tierra_2020_epsg3316_100k_2020_dissolved_optimized.tif\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Transform the vector files and convert them into rasters\"\"\" # We have to update this\n",
    "for file in optimized_vector_files_path[:]:\n",
    "    output_path_file = os.path.join(output_path, os.path.basename(file).replace(\".shp\", \".tif\"))\n",
    "    print(f\"{os.path.basename(file)} opened\")\n",
    "    print(\"Creating\" , output_path_file)\n",
    "    gdal_rasterize_from_shapefile(file, resolution, nodata_value, data_type, output_path_file, cols=None, rows=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filecolombia_cobertura_tierra_clc_epsg3116_30m_2011.tif shape is (65397, 54718)\n",
      "All raster files have the same dimensions\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check the dimensions of all the rasters\"\"\"\n",
    "raster_list = get_raster_file_list(output_path)\n",
    "check_same_dimensions(raster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: Y:\\z_resources\\ruben\\landcover_vector_files_copy\\illogical_files\n",
      "Comparing pixels at rows/columns (15616,58112) from (54718, 65397)\r"
     ]
    }
   ],
   "source": [
    "\"\"\"Create the illogical transitions 1 to 1 and vectorize it\"\"\"\n",
    "\n",
    "illogical_path = path + r\"\\illogical_files\"\n",
    "create_folder_if_not_exists(illogical_path)\n",
    "\n",
    "# rule_values_list = read_csv_in_pairs(rule_table_path)\n",
    "rule_values_dict = read_csv_in_trios(rule_table_path)\n",
    "raster_list = get_raster_file_list(output_path)\n",
    "all_pairs_path_list = generate_random_pairs(raster_list)\n",
    "\n",
    "\n",
    "for raster_pair in all_pairs_path_list[:]:\n",
    "    illogical_raster_path, illogical_df = compare_rasters(raster_pair, illogical_path, rule_table_path)\n",
<<<<<<< Updated upstream
    "    # illogical_df = add_name_columns_to_dataframe(illogical_df, column_name, names_dictionary)\n",
    "    \n",
=======
    "    # Not now\n",
    "    illogical_df = add_name_columns_to_dataframe(illogical_df, column_name, names_dictionary)\n",
    "    \n",
    "    # Not now\n",
>>>>>>> Stashed changes
    "    # vectorize_raster(illogical_raster_path, output_path, illogical_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing raster 1/3: Y:\\z_resources\\ruben\\landcover_vector_files_copy\\illogical_files\\2011_2018_illogical_transitions.tif\n"
     ]
    }
   ],
   "source": [
    "illogical_files_path = r\"Y:\\z_resources\\ruben\\landcover_vector_files_copy\\illogical_files\"\n",
    "raster_list = get_raster_file_list(illogical_files_path)\n",
    "\n",
    "# Initialize the dictionary to store unique sets and their corresponding unique IDs\n",
    "unique_sets = {}\n",
    "set_counter = 1  # Start assigning unique IDs from 1\n",
    "\n",
    "# Initialize the final array to store only the unique IDs for each pixel\n",
    "final_array = None\n",
    "\n",
    "# Loop through each raster in the raster_list\n",
    "for idx, raster_path in enumerate(raster_list):\n",
    "    print(f\"Processing raster {idx + 1}/{len(raster_list)}: {raster_path}\")\n",
    "    \n",
    "    with rasterio.open(raster_path) as src:\n",
    "        raster_data = src.read(1)  # Read the first (or only) band\n",
    "        raster_meta = src.meta     # Get metadata for output\n",
    "        \n",
    "        # Initialize the final array on the first iteration\n",
    "        if final_array is None:\n",
    "            # Initialize an empty array of zeros (will hold unique IDs)\n",
    "            final_array = np.zeros(raster_data.shape, dtype=np.uint32)\n",
    "            print(\"Initialized final_array to store unique IDs.\")\n",
    "\n",
    "        # Iterate over each pixel in the raster (i and j represent row and column)\n",
    "        for i in range(raster_data.shape[0]):\n",
    "            for j in range(raster_data.shape[1]):\n",
    "                \n",
    "                # Print status of the current pixel (to track progress)\n",
    "                if (i * raster_data.shape[1] + j) % 1000 == 0:  # Print every 1000 pixels processed\n",
    "                    print(f\"  Processing pixel ({i}, {j}) - Total pixels processed: {(i * raster_data.shape[1] + j)}\")\n",
    "\n",
    "                # Get the current value at pixel (i, j)\n",
    "                value = raster_data[i, j]\n",
    "\n",
    "                # Create the set for this pixel (include value)\n",
    "                if final_array[i, j] == 0:  # If this pixel hasn't been assigned an ID yet\n",
    "                    value_set = {value}  # Start with the current value as a set\n",
    "                else:\n",
    "                    # Add the value from the raster to the existing set of values at that pixel\n",
    "                    value_set = {value, *unique_sets.get(final_array[i, j], set())}\n",
    "                \n",
    "                # Convert set to frozenset (so it can be used as a dictionary key)\n",
    "                frozen_set = frozenset(value_set)\n",
    "                \n",
    "                # If this set hasn't been seen before, assign a new ID\n",
    "                if frozen_set not in unique_sets:\n",
    "                    unique_sets[frozen_set] = set_counter\n",
    "                    print(f\"    New unique set found: {value_set}. Assigned ID {set_counter}\")\n",
    "                    set_counter += 1\n",
    "                \n",
    "                # Assign the unique ID to the final array for this pixel\n",
    "                final_array[i, j] = unique_sets[frozen_set]\n",
    "\n",
    "    print(f\"Finished processing raster {raster_path}. Moving to next raster.\\n\")\n",
    "\n",
    "# After processing all rasters\n",
    "print(\"Finished processing all rasters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = final_array.shape\n",
    "pixel_values = []\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        pixel_values.append({\n",
    "            'row': i,\n",
    "            'col': j,\n",
    "            'values': list(final_array[i, j])  # Convert set to list\n",
    "        })\n",
    "\n",
    "# Convert the list to a DataFrame and save to CSV\n",
    "df = pd.DataFrame(pixel_values)\n",
    "df.to_csv('final_pixel_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build the year finder\"\"\"\n",
    "illogical_path = path + r\"\\illogical_files\"\n",
    "\n",
    "# Search for the input rasters\n",
    "raster_list = get_raster_file_list(output_path)\n",
    "years_list = [re.findall(r'\\d+', os.path.basename(file))[0] for file in raster_list] # We expect to only have one year per name\n",
    "year_pairs = [f\"{years_list[i]}_{years_list[i + 1]}\" for i in range(len(years_list) - 1)]\n",
    "\n",
    "illogical_rasters = get_raster_file_list(illogical_path)\n",
    "illogical_csvs = get_csv_file_list(illogical_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(year_pairs) - 1):   \n",
    "    raster_file_1 = [raster for raster in illogical_rasters if year_pairs[index] in raster][0]\n",
    "    raster_file_2 = [raster for raster in illogical_rasters if year_pairs[index + 1] in raster][0]\n",
    "\n",
    "    csv_1 = [raster for raster in illogical_csvs if year_pairs[index] in raster][0]\n",
    "    csv_2 = [raster for raster in illogical_csvs if year_pairs[index + 1] in raster][0]\n",
    "\n",
    "    df_1 = pd.read_csv(csv_1)\n",
    "    df_2 = pd.read_csv(csv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = set()\n",
    "data = set()\n",
    "with open(csv_1, 'r') as file:\n",
    "    header = next(file).strip().split(\",\")  # Extract header names\n",
    "    columns.update(header)  # Update columns set with header names\n",
    "    for line in file:\n",
    "        values = line.strip().split(\",\")\n",
    "        data.add(tuple(values))  # Add row as a tuple to the data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = read_csv_columns_and_data(csv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_df = pd.DataFrame()\n",
    "\n",
    "row_1 = df_1.loc[df_1[\"univalue\"] == value1].drop(columns=[\"univalue\"])\n",
    "row_2 = df_2.loc[df_2[\"univalue\"] == value2].drop(columns=[\"univalue\"])\n",
    "\n",
    "# Combine the rows into combination_df\n",
    "combination_df = pd.concat([row_1.reset_index(drop=True), row_2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Assign a unique value to \"univalue\" column\n",
    "combination_df[\"univalue\"] = len(combination_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to do a recursive function.\n",
    "for index in range(len(year_pairs) - 1):   \n",
    "    raster_file_1 = [raster for raster in illogical_rasters if year_pairs[index] in raster][0]\n",
    "    raster_file_2 = [raster for raster in illogical_rasters if year_pairs[index + 1] in raster][0]\n",
    "\n",
    "    csv_1 = [raster for raster in illogical_csvs if year_pairs[index] in raster][0]\n",
    "    csv_2 = [raster for raster in illogical_csvs if year_pairs[index + 1] in raster][0]\n",
    "\n",
    "    n = len(year_pairs) - 1\n",
    "\n",
    "    ds1 = gdal.Open(raster_file_1)\n",
    "    ds2 = gdal.Open(raster_file_2)\n",
    "\n",
    "    if not ds1 or not ds2:\n",
    "        print(\"Error: Unable to open raster files.\")\n",
    "        # return\n",
    "    \n",
    "    # Check if both rasters have the same height and width\n",
    "    if ds1.RasterXSize != ds2.RasterXSize or ds1.RasterYSize != ds2.RasterYSize:\n",
    "        print(f\"Error: Rasters have different dimensions.\")\n",
    "    \n",
    "    # Get the first raster information\n",
    "    width = ds1.RasterXSize\n",
    "    height = ds1.RasterYSize\n",
    "    geotransform = ds1.GetGeoTransform()\n",
    "    projection = ds1.GetProjection()\n",
    "    \n",
    "    # Read rule table and create a list of pairs with the info\n",
    "    df_1 = pd.read_csv(csv_1)\n",
    "    df_2 = pd.read_csv(csv_2)\n",
    "\n",
    "    # Create output raster\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    year1 = os.path.basename(year_pairs[0]).split('_')[1] \n",
    "    year2 = os.path.basename(year_pairs[1]).split('_')[1]\n",
    "    output_filename = f\"occsol_{n}_anat_illogical_transitions.tif\" #Customize\n",
    "    output_loc = os.path.join(output_path, output_filename)\n",
    "\n",
    "    output_ds = driver.Create(output_loc, width, height, 1, gdal.GDT_Int16, options= ['COMPRESS=DEFLATE', 'TILED=YES']) # GDT_Int32\n",
    "    output_ds.GetRasterBand(1).SetNoDataValue(0) # Set nodata value\n",
    "    output_ds.SetGeoTransform(geotransform)\n",
    "    output_ds.SetProjection(projection)\n",
    "\n",
    "    output_array = np.zeros((height, width), dtype=np.int16) # int16\n",
    "\n",
    "    combination_df = pd.DataFrame()\n",
    "\n",
    "    unique_value_dict = {} # pairs : unique_value\n",
    "    # Loop through each pixel and compare values\n",
    "    block_size = 256  # Adjust the block size as needed\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            print(f\"Comparing pixels at rows/columns ({x},{y}) from ({width}, {height})\") # , end='\\r'\n",
    "            block_width = min(block_size, width - x)\n",
    "            block_height = min(block_size, height - y)\n",
    "\n",
    "            block1 = ds1.GetRasterBand(1).ReadAsArray(x, y, block_width, block_height)\n",
    "            block2 = ds2.GetRasterBand(1).ReadAsArray(x, y, block_width, block_height)\n",
    "    \n",
    "            # Check if the pair of values matches any rule pair\n",
    "            correct_value = 0\n",
    "            \n",
    "            # rule_values_list = set(rule_values_list)\n",
    "\n",
    "            for i in range(block_height):\n",
    "                for j in range(block_width):\n",
    "                    value1 = block1[i, j]\n",
    "                    value2 = block2[i, j]\n",
    "\n",
    "                    if value1 != 0 or value2 != 0:\n",
    "                        print(f\"{value1}_{value2}passed if\")\n",
    "                        # value can be 0, so we have to fix that\n",
    "                        # print(row_1)\n",
    "                        # row_1 = df_1.loc[df_1[\"univalue\"] == value1].drop(columns=[\"univalue\"])\n",
    "                        # row_2 = df_2.loc[df_2[\"univalue\"] == value2].drop(columns=[\"univalue\"])\n",
    "\n",
    "                        # # Combine the rows into combination_df\n",
    "                        # combination_df = pd.concat([row_1.reset_index(drop=True), row_2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "                        # # Serialize the combined DataFrame to a JSON string\n",
    "                        # combination_json = combination_df.to_json()\n",
    "\n",
    "                        # unique_value = unique_value_dict.setdefault(combination_json, len(unique_value_dict) + 1)\n",
    "\n",
    "\n",
    "                        output_array[y + i, x + j] = 1\n",
    "                    else:\n",
    "                        output_array[y + i, x + j] = correct_value\n",
    "\n",
    "    output_ds.GetRasterBand(1).WriteArray(output_array)\n",
    "    # Close datasets\n",
    "    ds1 = None\n",
    "    ds2 = None\n",
    "    output_ds = None\n",
    "\n",
    "\n",
    "    # return output_loc, comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"IN PROGRESS Create illogical transitions altogether and vectorize it\"\"\"\n",
    "rule_values_list = read_csv(rule_table_path)\n",
    "all_pairs_path_list = consecutive_pairs(raster_list)\n",
    "\n",
    "# Get the first raster pair\n",
    "for raster_pair in all_pairs_path_list[0:]:\n",
    "    illogical_raster_path, illogical_df = compare_rasters(raster_pair, output_path, rule_table_path)\n",
    "\n",
    "# Union the create vector files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_rasters(input_folder): # Esto ya no sirve para mucho pero no quiero borrarlo\n",
    "    \"\"\"Provides two lists of files that go in pairs\"\"\"\n",
    "    # Get a list of raster files in the input folder\n",
    "    raster_files = [filename for filename in os.listdir(input_folder) if filename.endswith((\".tif\", \".tiff\"))]\n",
    "\n",
    "    # Create two empty lists\n",
    "    first_file_list = []\n",
    "    second_file_list = []\n",
    "\n",
    "    # Iterate through pairs of current and next files\n",
    "    for i in range(len(raster_files) - 1):\n",
    "        current_file = os.path.join(input_folder, raster_files[i])\n",
    "        next_file = os.path.join(input_folder, raster_files[i + 1])\n",
    "        # If the second file is empty, finish\n",
    "        if not next_file:\n",
    "            print(\"Finished getting both values\")\n",
    "            continue\n",
    "        else:\n",
    "            first_file_list.append(current_file)\n",
    "            second_file_list.append(next_file)\n",
    "\n",
    "    return first_file_list, second_file_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_forge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
