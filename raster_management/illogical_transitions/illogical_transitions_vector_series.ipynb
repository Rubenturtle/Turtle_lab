{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illogical transitions as a series\n",
    "- This script gets a batch of vector files located in a folder and creates a single output wrapping all the series identifying where and what illogical transitions happen in all the AOI.\n",
    "\n",
    "- Inputs\n",
    "    - Path of the folder with the vector files.\n",
    "    - Path of the csv with all the illogical transitions (val1, val2) without headers.\n",
    "    - Column reference name of the vector files for rasterization.\n",
    "    - Path of the csv file without headers containing or:\n",
    "        - 1 column table with all the accurate names.\n",
    "        - 2 column table with the values and its corresponding names.\n",
    "- Outputs\n",
    "    - Path of the output folder with the final vector file.\n",
    "    - Path of all the intermediate processing files.\n",
    "- Processing\n",
    "    - Read all the vector input files.\n",
    "    - Optional: Dissolve them to make the rest of the process easier.\n",
    "    - Check the input column if it is based on values or text.\n",
    "    - Create the corresponding dictionary for classification.\n",
    "    - Homogenize the reference column with the dictionary.\n",
    "    - Create intermediate vector files.\n",
    "    - Rasterize the reference column.\n",
    "    - Compare all the years in pairs creating the corresponding illogical files.\n",
    "        - Raster files with ID values.\n",
    "        - csv with ID values and pair accumulated values.\n",
    "    - Create a folder system with all the required iterations.\n",
    "    - Compare all the files inside the folder system.\n",
    "    - Get all the generated csvs.\n",
    "    - Wrap up all the info in a single table.\n",
    "    - Append the text info to the table.\n",
    "    - Vectorize the final output.\n",
    "- Author\n",
    "    - Rubén Crespo Ceballos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_if_not_exists(folder_path):\n",
    "    \"\"\"\n",
    "    Create a folder if it doesn't exist.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path of the folder to be created.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder created at: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Folder already exists at: {folder_path}\")\n",
    "\n",
    "def get_vector_file_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of the vector files inside the folder\n",
    "    Parameters:\n",
    "    - path (str): path of the folder with the resources.\n",
    "\n",
    "    Returns:\n",
    "    - File_list (list). list of the resources.\n",
    "    \"\"\"\n",
    "    File_list = [] #f for f in os.listdir(path) if os.isfile(mypath,f)\n",
    "    for file in os.listdir(path):\n",
    "        # \"anat\" is just to get here necessary ones\n",
    "        if file.endswith(\".shp\"):\n",
    "            if file not in File_list:\n",
    "                File_list.append(os.path.join(path,file))\n",
    "        else:\n",
    "            pass\n",
    "    return File_list\n",
    "\n",
    "def get_raster_file_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of the raster files inside the folder\n",
    "    Parameters:\n",
    "    - path (str): path of the folder with the resources.\n",
    "\n",
    "    Returns:\n",
    "    - File_list (list). list of the resources.\n",
    "    \"\"\"\n",
    "    File_list = [] #f for f in os.listdir(path) if os.isfile(mypath,f)\n",
    "    for file in os.listdir(path):\n",
    "        # \"32628\" is just to get here necessary ones\n",
    "        if file.endswith(\".tif\") or file.endswith(\".tiff\"):\n",
    "            if file not in File_list:\n",
    "                File_list.append(os.path.join(path,file))\n",
    "        else:\n",
    "            pass\n",
    "    return File_list\n",
    "\n",
    "def get_csv_file_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of the csv files inside the folder\n",
    "    Parameters:\n",
    "    - path (str): path of the folder with the resources.\n",
    "\n",
    "    Returns:\n",
    "    - File_list (list). list of the resources.\n",
    "    \"\"\"\n",
    "    File_list = [] #f for f in os.listdir(path) if os.isfile(mypath,f)\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            if file not in File_list:\n",
    "                File_list.append(os.path.join(path,file))\n",
    "        else:\n",
    "            pass\n",
    "    return File_list\n",
    "\n",
    "def update_names_based_on_similarity(unique_names, gdf, column_name, similarity_threshold=0):\n",
    "    \"\"\"\n",
    "    Update names in gdf based on similarity to names in unique list.\n",
    "\n",
    "    Parameters:\n",
    "    - unique_names (list): list of the unique names.\n",
    "    - gdf (GeoDataFrame): GeoDataFrame whose names need to be updated.\n",
    "    - column_name (str): String of the column.\n",
    "    - similarity_threshold (float): Threshold for similarity ratio.\n",
    "\n",
    "    Returns:\n",
    "    - gdf. Updates gdf in place.\n",
    "    \"\"\"\n",
    "    # Add a new column 'valid_text' with None values\n",
    "    gdf['valid_text'] = None\n",
    "\n",
    "    total_elements = len(gdf)  # Get total number of elements\n",
    "\n",
    "    # Iterate through rows of gdf2\n",
    "    for index, row in gdf.iterrows():\n",
    "\n",
    "        # Get the value of the column for the current row\n",
    "        name_gdf = row[column_name]\n",
    "        highest_similarity_ratio = 0\n",
    "        best_matching_name = None\n",
    "        # Iterate through unique names in gdf1\n",
    "        for unique_name in unique_names:\n",
    "            # Calculate similarity ratio between names in gdf2 and gdf1\n",
    "            similarity_ratio = SequenceMatcher(None, unique_name, name_gdf).ratio()\n",
    "            # Update best matching name if similarity ratio is higher\n",
    "            if similarity_ratio > highest_similarity_ratio:\n",
    "                highest_similarity_ratio = similarity_ratio\n",
    "                best_matching_name = unique_name\n",
    "\n",
    "        if highest_similarity_ratio >= similarity_threshold:\n",
    "            gdf.at[index, 'valid_text'] = best_matching_name\n",
    "            gdf.at[index, 'simil_val'] = highest_similarity_ratio\n",
    "\n",
    "        print(f\"Processing element {index + 1}/{total_elements}\", end=\"\\r\") # This is to track the process\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def create_identifier_dictionary(list):\n",
    "    \"\"\"\n",
    "    Creates a dictionary out of a list appending a new id to each one of them.\n",
    "\n",
    "    Parameters:\n",
    "    - list (list): list of strings.\n",
    "\n",
    "    Returns:\n",
    "    - value_to_text_dict. dictionary o value: text.\n",
    "    \"\"\"\n",
    "    value_to_text_dict = {index + 1: value for index, value in enumerate(sorted(list))} # {val: text}\n",
    "    return value_to_text_dict\n",
    "\n",
    "\n",
    "\n",
    "def gdal_rasterize_from_shapefile(shapefile_path, resolution, nodata_value, data_type, output_path, cols=None, rows=None):\n",
    "    \"\"\"\n",
    "    Rasterizes a GeoDataFrame using GDAL directly.\n",
    "\n",
    "    Parameters:\n",
    "    - shapefile_path (string): path of the vector file.\n",
    "    - resolution (int or float): Resolution of the raster (pixel size).\n",
    "    - nodata_value: The value to use for no-data pixels.\n",
    "    - data_type: Data type for the output raster (e.g., gdal.GDT_Float32).\n",
    "    - output_path (str): Path to save the output raster.\n",
    "    - cols (int, optional): Number of columns in the output raster.\n",
    "    - rows (int, optional): Number of rows in the output raster.\n",
    "\n",
    "    Returns:\n",
    "    - None. The function writes the raster to the specified output path.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the Shapefile using OGR\n",
    "    shapefile = ogr.Open(shapefile_path)\n",
    "    layer = shapefile.GetLayer()\n",
    "\n",
    "    # Get the bounds of the Shapefile (same as GeoDataFrame's total_bounds)\n",
    "    xmin, xmax, ymin, ymax = layer.GetExtent()\n",
    "\n",
    "    # If cols and rows are not provided, calculate them based on resolution\n",
    "    if cols is None or rows is None:\n",
    "        cols = int((xmax - xmin) / resolution)\n",
    "        rows = int((ymax - ymin) / resolution)\n",
    "\n",
    "    # Create a new raster dataset\n",
    "    raster_ds = gdal.GetDriverByName('GTiff').Create(\n",
    "        output_path, cols, rows, 1, data_type,\n",
    "        options=['COMPRESS=DEFLATE', 'TILED=YES']\n",
    "    )\n",
    "\n",
    "    # Set the geotransform (affine transform for the raster)\n",
    "    geotransform = (xmin, resolution, 0, ymax, 0, -resolution)\n",
    "    raster_ds.SetGeoTransform(geotransform)\n",
    "\n",
    "    # Set the CRS (coordinate reference system) from the Shapefile\n",
    "    srs = layer.GetSpatialRef()\n",
    "    if srs:\n",
    "        raster_ds.SetProjection(srs.ExportToWkt())\n",
    "\n",
    "    # Create the raster band and set no-data value\n",
    "    band = raster_ds.GetRasterBand(1)\n",
    "    band.SetNoDataValue(nodata_value)\n",
    "\n",
    "    # Rasterize the shapefile\n",
    "    gdal.RasterizeLayer(\n",
    "        raster_ds,  # Output raster dataset\n",
    "        [1],        # Raster band to write to\n",
    "        layer,      # Input OGR layer to rasterize\n",
    "        options=['ATTRIBUTE=raster_val', 'ALL_TOUCHED=TRUE']\n",
    "    )\n",
    "\n",
    "    # Flush and close the raster dataset\n",
    "    band.FlushCache()\n",
    "    raster_ds = None  # Close the file and save\n",
    "\n",
    "    # Close the shapefile\n",
    "    shapefile = None\n",
    "\n",
    "    print(f\"Rasterization complete: {output_path}\")\n",
    "\n",
    "    return cols, rows\n",
    "\n",
    "def check_same_dimensions(raster_files):\n",
    "    \"\"\"\n",
    "    Check the if the dimensions of all the input rasters have the same dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_files (list): List of raster files.\n",
    "\n",
    "    Returns:\n",
    "    - dimensions_list (list). The list with all the dimensions.\n",
    "    \"\"\"\n",
    "    dimensions_list = []\n",
    "\n",
    "    # Open the first raster file in the list\n",
    "    for file_path in raster_files[:]:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            shape_dimensions = src.shape\n",
    "            dimensions_list.append(shape_dimensions)\n",
    "\n",
    "    if len(set(dimensions_list)) == 1:\n",
    "        print(f\"All the elements have the same dimensions{dimensions_list[0]}\")\n",
    "        dimensions_list\n",
    "    else:\n",
    "        print(\"The dimensions are note the same\")\n",
    "        return dimensions_list\n",
    "\n",
    "\n",
    "def read_csv_in_pairs(csv_file):\n",
    "    \"\"\"\n",
    "    Reads a two column csv and transforms it into a pair value list.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file (str): path of the csv file.\n",
    "\n",
    "    Returns:\n",
    "    - rule_values_list: list of unique value pairs.\n",
    "    \"\"\"\n",
    "    rule_values_list = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        # next(file)  # Skip the header row\n",
    "        for line in file:\n",
    "            # Split the line into two values based on spaces, and convert them to floats\n",
    "            rule_value_1, rule_value_2 = map(float, line.split(\",\"))\n",
    "            rule_values_list.append((rule_value_1, rule_value_2))\n",
    "    return rule_values_list\n",
    "\n",
    "def csv_to_dict(file_path):\n",
    "    \"\"\"\n",
    "    Creates a dictionary out of a csv of two columns excluding the header.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (string): path of the file.\n",
    "\n",
    "    Returns:\n",
    "    - result_dict(dict). dictionary o value: text.\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "\n",
    "    # Open the CSV file\n",
    "    with open(file_path, mode='r', newline='', encoding='ISO-8859-1') as file: # encoding='ISO-8859-1' encoding='utf-8'\n",
    "        reader = csv.reader(file)\n",
    "\n",
    "        # Skip the header\n",
    "        # next(reader)\n",
    "\n",
    "        # Iterate through each row and add to the dictionary\n",
    "        for row in reader:\n",
    "            key = row[1]  # First column as the key\n",
    "            value = row[0]  # Second column as the value\n",
    "            result_dict[float(key)] = value\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def csv_to_list(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with one column and transforms it into a list.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (string): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - return(list): List containing the values from the column\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    with open(file_path, mode='r', newline='', encoding='ISO-8859-1') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row:  # Ensure the row is not empty\n",
    "                result.append(row[0])\n",
    "    return result\n",
    "\n",
    "def generate_consecutive_pairs(paths_list):\n",
    "    \"\"\"\n",
    "    Generates a list of consecutive pairs from the input list in order.\n",
    "\n",
    "    Parameters:\n",
    "    - paths_list (list): list of paths.\n",
    "\n",
    "    Returns:\n",
    "    - consecutive_pairs: list of consecutive value pairs.\n",
    "    \"\"\"\n",
    "    # Create consecutive pairs using zip\n",
    "    consecutive_pairs = list(zip(paths_list, paths_list[1:]))\n",
    "    return consecutive_pairs\n",
    "\n",
    "def compare_rasters(raster_pair, output, rule_values_list = None):\n",
    "    \"\"\"\n",
    "    Generates a list of random unique pairs, from the input list.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_pair (list): list of two elements.\n",
    "    - output (path): path of the output file.\n",
    "\n",
    "    Returns:\n",
    "    - output_loc (str): path out the ourput file.\n",
    "    - comparison_df: dataframe related to the data.\n",
    "    \"\"\"\n",
    "    # Open raster files\n",
    "    ds1 = gdal.Open(raster_pair[0])\n",
    "    ds2 = gdal.Open(raster_pair[1])\n",
    "\n",
    "    if not ds1 or not ds2:\n",
    "        print(\"Error: Unable to open raster files.\")\n",
    "        return\n",
    "\n",
    "    # Check if both rasters have the same height and width\n",
    "    if ds1.RasterXSize != ds2.RasterXSize or ds1.RasterYSize != ds2.RasterYSize:\n",
    "        print(\"Error: Rasters have different dimensions.\")\n",
    "\n",
    "    # Get the first raster information\n",
    "    width = ds1.RasterXSize\n",
    "    height = ds1.RasterYSize\n",
    "    geotransform = ds1.GetGeoTransform()\n",
    "    projection = ds1.GetProjection()\n",
    "\n",
    "    # Create output raster\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    layer_1 = os.path.basename(raster_pair[0]).split('_')[-1].replace(\".tif\",\"\") # Get always the last element\n",
    "    layer_2 = os.path.basename(raster_pair[1]).split('_')[-1].replace(\".tif\",\"\")\n",
    "    output_filename = f\"illogical_transitions_{layer_1}-{layer_2}.tif\" #Customize\n",
    "    output_loc = os.path.join(output, output_filename)\n",
    "\n",
    "    output_ds = driver.Create(output_loc, width, height, 1, gdal.GDT_Int16, options= ['COMPRESS=DEFLATE', 'TILED=YES']) # GDT_Int32\n",
    "    output_ds.GetRasterBand(1).SetNoDataValue(0)\n",
    "    output_ds.SetGeoTransform(geotransform)\n",
    "    output_ds.SetProjection(projection)\n",
    "\n",
    "    output_array = np.zeros((height, width), dtype=np.int16) # int16\n",
    "\n",
    "    unique_value_dict = {} # pairs : unique_value\n",
    "\n",
    "    # Loop through each pixel and compare values\n",
    "    block_size = 256  # Adjust the block size as needed\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            print(f\"Comparing pixels at rows/columns ({x},{y}) from ({width}, {height})\", end='\\r')\n",
    "            block_width = min(block_size, width - x)\n",
    "            block_height = min(block_size, height - y)\n",
    "\n",
    "            block1 = ds1.GetRasterBand(1).ReadAsArray(x, y, block_width, block_height)\n",
    "            block2 = ds2.GetRasterBand(1).ReadAsArray(x, y, block_width, block_height)\n",
    "\n",
    "            correct_value = 0\n",
    "\n",
    "            if rule_values_list:\n",
    "                rule_values_list = set(rule_values_list) # To speed up the process, don't use list\n",
    "                for i in range(block_height):\n",
    "                    for j in range(block_width):\n",
    "                        value1 = block1[i, j]\n",
    "                        value2 = block2[i, j]\n",
    "\n",
    "                        if (value1, value2) in rule_values_list:\n",
    "                            unique_value = unique_value_dict.setdefault((value1, value2), len(unique_value_dict) + 1)\n",
    "                            output_array[y + i, x + j] = unique_value\n",
    "                        else:\n",
    "                            output_array[y + i, x + j] = correct_value\n",
    "            else:\n",
    "                for i in range(block_height):\n",
    "                    for j in range(block_width):\n",
    "                        value1 = block1[i, j]\n",
    "                        value2 = block2[i, j]\n",
    "\n",
    "                        if value1 or value2:\n",
    "                            unique_value = unique_value_dict.setdefault((value1, value2), len(unique_value_dict) + 1)\n",
    "                            output_array[y + i, x + j] = unique_value\n",
    "                        else:\n",
    "                            output_array[y + i, x + j] = correct_value\n",
    "\n",
    "\n",
    "    # Write the output\n",
    "    output_ds.GetRasterBand(1).WriteArray(output_array)\n",
    "\n",
    "    # Close datasets\n",
    "    ds1 = None\n",
    "    ds2 = None\n",
    "    output_ds = None\n",
    "\n",
    "    # Create the dataframe\n",
    "    unique_values = list(unique_value_dict.values())\n",
    "    value_pairs = list(unique_value_dict.keys())\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'unival': unique_values,\n",
    "        f'{layer_1}': [pair[0] for pair in value_pairs],\n",
    "        f'{layer_2}': [pair[1] for pair in value_pairs]\n",
    "                                })\n",
    "\n",
    "    # Lets export the csv just in case\n",
    "    comparison_df.to_csv(os.path.join(output, f\"illo_tab_{layer_1}-{layer_2}.csv\"), index=False)\n",
    "\n",
    "    return print(\"Finished with: \", output_loc)\n",
    "\n",
    "def vectorize_raster(raster_path, output_path, df):\n",
    "    \"\"\"\n",
    "    Vectorizes an input raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): path of the input raster.\n",
    "    - output_path (path): path of the output file.\n",
    "    - df (dataframe): dataframe related to the input raster.\n",
    "\n",
    "    Returns:\n",
    "    - None. It produces the vector file.\n",
    "    \"\"\"\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Read raster data as numpy array\n",
    "        data = src.read(1)\n",
    "        # Get affine transform of the raster\n",
    "        transform = src.transform\n",
    "\n",
    "    # Vectorize the raster data\n",
    "    raster_shapes = features.shapes(data, transform=transform)\n",
    "\n",
    "    # Convert the vectorized shapes into a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame.from_features(\n",
    "        [\n",
    "            {\"geometry\": geo_shape, \"properties\": {\"ID\": value}}\n",
    "            for geo_shape, value in raster_shapes\n",
    "        ],\n",
    "        crs=src.crs\n",
    "    )\n",
    "\n",
    "    # Merge the dataframe with the GeoDataFrame\n",
    "    merged_gdf = gdf.merge(df, on='ID')\n",
    "    # Dissolve based on a column value\n",
    "    dissolved_gdf = merged_gdf.dissolve(by='ID')\n",
    "\n",
    "    # Save the merged GeoDataFrame as a new shapefile\n",
    "    filename = os.path.join(output_path, \"illogical_transitions\")\n",
    "    dissolved_gdf.to_file(f'{filename}.shp', driver='ESRI Shapefile')\n",
    "    return\n",
    "\n",
    "def add_name_columns_to_dataframe(df, names_dictionary):\n",
    "    \"\"\"\n",
    "    Adds the input name list into the  an input raster.\n",
    "\n",
    "    Parameters:\n",
    "    - df (dataframe): dataframe related to the input raster.\n",
    "    - names_dictionary (dict): dict of the names that we want to append.\n",
    "\n",
    "    Returns:\n",
    "    - df (dataframe). It produces the updated dataframe with the names.\n",
    "    \"\"\"\n",
    "    # Get the original columns (without including any new columns)\n",
    "    original_columns = df.columns.tolist()\n",
    "\n",
    "    for column in original_columns[1:]: # Skip the first column\n",
    "        df[column + \"txt\"] = df[column].map(names_dictionary)\n",
    "        # Alternative\n",
    "        # column_list = column.split(\"_\") # Example 2019_1\n",
    "        # df[column_list[0] + \"_text_\" + column_list[1]] = df[column].map(names_dictionary)\n",
    "\n",
    "    # Sort the columns\n",
    "    # order_columns = [df.columns[0]] + sorted(df.columns[1:])\n",
    "    # df = df[order_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "def dissolve_geodataframe(gdf, column):\n",
    "    \"\"\"\n",
    "    Dissolves a GeoDataFrame based on unique values of a specified column.\n",
    "\n",
    "    Parameters:\n",
    "    gdf (GeoDataFrame): Input GeoDataFrame to be dissolved.\n",
    "    column (str): Column name based on which to dissolve the GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: The dissolved GeoDataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Initial number of geometries: {len(gdf)}\")\n",
    "\n",
    "    # Perform the dissolve operation\n",
    "    dissolved_gdf = gdf.dissolve(by=column)\n",
    "\n",
    "    print(f\"Final number of geometries after dissolve: {len(dissolved_gdf)}\")\n",
    "\n",
    "    return dissolved_gdf\n",
    "\n",
    "def process_folders(iteration_folders, idx=0, df=None, aggregated_df=None):\n",
    "    \"\"\"\n",
    "    Recursively processes a series of folders containing CSV files to merge data based on shared columns.\n",
    "\n",
    "    Parameters:\n",
    "    iteration_folders (list): A list of folder paths to iterate through, each containing CSV files.\n",
    "    idx (int): The current folder index being processed. Defaults to 0.\n",
    "    df (DataFrame): The current DataFrame being merged. Defaults to None for the first iteration.\n",
    "    aggregated_df (DataFrame): The cumulative aggregated DataFrame. Defaults to None for the first iteration.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The final aggregated DataFrame containing merged data from all folders.\n",
    "\n",
    "    Functionality:\n",
    "    - Reads the initial CSV from the first folder and initializes the aggregation process.\n",
    "    - Iterates through columns in the current DataFrame (`df`), identifying corresponding CSV files in the next folder.\n",
    "    - Merges data from matching files into the current DataFrame using inner joins, while retaining unmatched rows with outer joins.\n",
    "    - Adds unique, incrementing suffixes to column names to avoid duplicates.\n",
    "    - Recursively processes subsequent folders, updating `aggregated_df` with new merged data at each step.\n",
    "    - Ensures columns are reordered to keep 'ID' first and all other columns sorted alphabetically.\n",
    "    - Stops when all folders are processed.\n",
    "    \"\"\"\n",
    "    if idx >= len(iteration_folders) - 1:\n",
    "        return aggregated_df\n",
    "\n",
    "    current_folder = iteration_folders[idx]\n",
    "    next_folder = iteration_folders[idx + 1]\n",
    "\n",
    "    if df is None and aggregated_df is None:\n",
    "        # Read the initial csv\n",
    "        csv_files = [f for f in os.listdir(current_folder) if f.endswith('.csv')]\n",
    "        file_path = os.path.join(current_folder, csv_files[0])\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Create the aggregated_df\n",
    "        df = df.rename(columns={'unival': 'ID'})\n",
    "        unival_series = df['ID']\n",
    "        aggregated_df = pd.DataFrame(unival_series.to_frame())\n",
    "\n",
    "    column_names = df.columns[1:].tolist() # we dont want the first column\n",
    "\n",
    "    counter = 1  # Restart the counter at 1 for each folder iteration\n",
    "\n",
    "    for column_name in column_names:\n",
    "        # The next iterations names come with a suffix. Remove it for searching\n",
    "        if \"_\" in column_name:\n",
    "            base_column_name = column_name.split(\"_\")[0]\n",
    "        else:\n",
    "            base_column_name = column_name\n",
    "\n",
    "        # Look for the matching CSV in the next folder, search and open the csv\n",
    "        csv_files = [f for f in os.listdir(next_folder) if f.endswith('.csv')]\n",
    "        matching_csv = [file for file in csv_files if file.startswith(base_column_name)][0] # we expect only one element\n",
    "        if not matching_csv:\n",
    "            raise FileNotFoundError(f\"No matching file found for {base_column_name} in {next_folder}\")\n",
    "        matching_path = os.path.join(next_folder, matching_csv)\n",
    "        match_df = pd.read_csv(matching_path)\n",
    "\n",
    "        # Add a suffix to the columns so we can diferenciate then. Increment by the number of columns added to match_df (excluding the first column)\n",
    "        match_df.columns = [match_df.columns[0]] + [f'{col}_{counter}' for counter, col in enumerate(match_df.columns[1:], start=counter)]\n",
    "        counter += len(match_df.columns) - 1\n",
    "\n",
    "        # Merge the data (internal)\n",
    "        df = df.rename(columns={column_name: base_column_name}) # Rename the column to do the merge.\n",
    "        merged_df = pd.merge(df, match_df, left_on=base_column_name, right_on=\"unival\", how='outer') #outer means that we keep all the combinations\n",
    "\n",
    "        # THIS IS OPTIONAL: Fill NAN to 0.\n",
    "        merged_df = merged_df.fillna(0)\n",
    "\n",
    "        # To surpass duplicate names\n",
    "        merged_df = merged_df.drop(['unival', base_column_name], axis=1)\n",
    "        merged_df = merged_df.drop(columns=column_names, errors='ignore')\n",
    "\n",
    "        df = df.rename(columns={base_column_name: column_name}) # We put the name back\n",
    "\n",
    "        # Merge with aggregated_df (external)\n",
    "        aggregated_df = pd.merge(merged_df, aggregated_df, on='ID', how='inner')\n",
    "\n",
    "    # Drop columns that were used for merging in the current iteration\n",
    "    aggregated_df = aggregated_df.drop(columns=column_names, errors='ignore')\n",
    "\n",
    "    # Reorder columns after the first column\n",
    "    order_columns = [aggregated_df.columns[0]] + sorted(aggregated_df.columns[1:], key=lambda x: int(x.split('_')[1])) # Remove after \", key=...\" if any issue arisses\n",
    "    aggregated_df = aggregated_df[order_columns]\n",
    "    return process_folders(iteration_folders, idx=idx + 1, df=aggregated_df, aggregated_df=aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\output_tmp_path\n",
      "Folder created at: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\output_path\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Specify all the inputs\"\"\"\n",
    "# Path for the vector inputs\n",
    "input_path = r\"Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\"\n",
    "\n",
    "# Path for the final output\n",
    "output_path = r\"Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\output_path\"\n",
    "\n",
    "# Path of the illogical rules csv\n",
    "rule_table_path = r\"Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\illogical_transitions.csv\"\n",
    "\n",
    "#Path of all the intermediate outputs (Don't touch this)\n",
    "output_tmp_path = input_path + r\"\\output_tmp_path\"\n",
    "\n",
    "#Create the paths\n",
    "create_folder_if_not_exists(output_tmp_path)\n",
    "create_folder_if_not_exists(output_path)\n",
    "\n",
    "\"\"\"We have here two situations. Comment the non necessary one\"\"\"\n",
    "\n",
    "# If we have only strings with no values. We append a created value.\n",
    "names_list_path = r\"Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\names_list.csv\"\n",
    "names_list = csv_to_list(names_list_path)\n",
    "names_dictionary = create_identifier_dictionary(names_list) # The output will always be number: text \n",
    "\n",
    "# If we have both strings and values\n",
    "names_dictionary_path = r\"Y:\\z_resources\\ruben\\landcover_vector_files_copy\\names_dictionary.csv\" #It has headers\n",
    "names_dictionary = csv_to_dict(names_dictionary_path) # {Code: label}\n",
    "names_list = list(names_dictionary.values())\n",
    "\n",
    "vector_file_list = get_vector_file_list(input_path)\n",
    "\n",
    "\n",
    "\"\"\"For rasterization\"\"\"\n",
    "# Specify the column to rasterize by.\n",
    "column_name = 'leyenda'\n",
    "# Define the resolution of your raster.\n",
    "resolution = 30  # in meters\n",
    "# Define the nodata value of your raster.\n",
    "nodata_value = 0\n",
    "# Define the data type of the raster.\n",
    "data_type = gdal.GDT_UInt16\n",
    "\"\"\"\n",
    "gdal.GDT_Byte,\n",
    "gdal.GDT_Int16,\n",
    "gdal.GDT_UInt16,\n",
    "gdal.GDT_Int32,\n",
    "gdal.GDT_UInt32,\n",
    "gdal.GDT_Float32,\n",
    "gdal.GDT_Float64\n",
    "\"\"\"\n",
    "# Define the rows and columns for the rasterization (All the files must have the same dimensions).\n",
    "# You can put the parameters here manually, or let the system take the reference of the first rasterization.\n",
    "rows = None\n",
    "columns = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OPTIONAL: For heavy vector files, we are going to dissolve based on the designed column, to simplify the rest of the process\"\"\"\n",
    "dissolved_files_path = output_tmp_path + r\"\\dissolved_files\"\n",
    "create_folder_if_not_exists(dissolved_files_path)\n",
    "\n",
    "for file in vector_file_list[:]:\n",
    "    gdf = gpd.read_file(file)\n",
    "    print(\"gdf opened\")\n",
    "    gdf = dissolve_geodataframe(gdf, column_name)\n",
    "    gdf.to_file(os.path.join(dissolved_files_path, os.path.basename(file).replace(\".shp\", \"_dissolved.shp\")) , driver='ESRI Shapefile')\n",
    "    \n",
    "# Read the new imputs\n",
    "vector_file_list = get_vector_file_list(dissolved_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the \"raster_val\" for rasterization according to the content of the columnn\"\"\"\n",
    "optimized_vector_files_path = output_tmp_path + r\"\\optimized_vector_files\"\n",
    "create_folder_if_not_exists(optimized_vector_files_path)\n",
    "\n",
    "# Create a list to store filtered DataFrames\n",
    "filtered_dfs = []\n",
    "\n",
    "for file in vector_file_list[:]:\n",
    "    gdf = gpd.read_file(file)\n",
    "    print(f\"gdf {os.path.basename(file)} opened\")\n",
    "    if gdf[column_name].dtype == object:\n",
    "        print(f\"The column '{column_name}' contains strings.\")\n",
    "        gdf = update_names_based_on_similarity(names_list, gdf, column_name, similarity_threshold=0.3)\n",
    "        print(\"names updated\")\n",
    "        # Add a new column to the GeoDataFrame containing the unique identifiers\n",
    "        gdf['raster_val'] = gdf[\"valid_text\"].map(names_dictionary)\n",
    "        \n",
    "        \"\"\"Wrap the info into a df.\"\"\" # ACHTUNG: Pending to test\n",
    "        df = gdf.drop(columns=[\"geometry\"])\n",
    "\n",
    "        # Get unique rows based on the \"Valid_test\" column\n",
    "        unique_valid_texts = df[\"Valid_text\"].unique()\n",
    "\n",
    "        for value in unique_valid_texts:\n",
    "            # Filter rows with the current unique value\n",
    "            filtered_df = df[df[\"Valid_text\"] == value]\n",
    "            filtered_dfs.append(filtered_df)\n",
    "\n",
    "        # Merge all filtered DataFrames into one\n",
    "        merged_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        print(f\"The column '{column_name}' contains values.\")\n",
    "        # Get unique values/strings from the specified column, they are always sorted.\n",
    "        unique_values = sorted(gdf[column_name].unique())\n",
    "        # Create a dictionary one to one\n",
    "        value_to_index = {value: value for value in unique_values}\n",
    "        gdf['raster_val'] = gdf[column_name].map(value_to_index)\n",
    "    \n",
    "    # Opening and saving the file takes a lot of time\n",
    "    gdf.to_file(os.path.join(optimized_vector_files_path, os.path.basename(file).replace(\".shp\", \"_optimized.shp\")) , driver='ESRI Shapefile')\n",
    "\n",
    "    # Saving the csv.\n",
    "    merged_df.to_csv(os.path.join(optimized_vector_files_path, 'optimited.csv'), index=False) \n",
    "    \n",
    "# Read the new imputs\n",
    "vector_file_list = get_vector_file_list(optimized_vector_files_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:\\z_resources\\ruben\\landcover_vector_files_copy\\output_files\\cobertura_tierra_2020_epsg3316_100k_2020_dissolved_optimized.tif\n",
      "Rasterization complete: Y:\\z_resources\\ruben\\landcover_vector_files_copy\\output_files\\cobertura_tierra_2020_epsg3316_100k_2020_dissolved_optimized.tif\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Transform the vector files and convert them into rasters\"\"\"\n",
    "rasterized_files_path = output_tmp_path + r\"\\rasterized_files_path\"\n",
    "create_folder_if_not_exists(rasterized_files_path)\n",
    "\n",
    "for file in vector_file_list[:]:\n",
    "    output_path_file = os.path.join(rasterized_files_path, os.path.basename(file).replace(\".shp\", \".tif\"))\n",
    "    print(f\"{os.path.basename(file)} opened\")\n",
    "    print(\"Creating\" , output_path_file)\n",
    "    cols, rows = gdal_rasterize_from_shapefile(file, resolution, nodata_value, data_type, output_path_file, cols=None, rows=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the elements have the same dimensions(9670, 13385)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check the dimensions of the rasters are all the same\"\"\"\n",
    "raster_list = get_raster_file_list(rasterized_files_path)\n",
    "check_same_dimensions(raster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the illogical transitions 1 to 1\"\"\"\n",
    "\n",
    "illogical_path = output_tmp_path + r\"\\illogical_files\"\n",
    "create_folder_if_not_exists(illogical_path)\n",
    "\n",
    "raster_list = get_raster_file_list(rasterized_files_path)\n",
    "all_pairs_path_list = generate_consecutive_pairs(raster_list)\n",
    "\n",
    "# Read rule table and create a list of pairs with the info\n",
    "rule_values_list = read_csv_in_pairs(rule_table_path)\n",
    "\n",
    "for raster_pair in all_pairs_path_list[:]:\n",
    "    compare_rasters(raster_pair, illogical_path, rule_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\output_tmp_path\\iteration_files_1\n",
      "Folder already exists at: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\output_tmp_path\\iteration_files_2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create the folder system\"\"\"\n",
    "# Delte this when finishing\n",
    "illogical_path = output_tmp_path + r\"\\illogical_files\"\n",
    "raster_list = get_raster_file_list(illogical_path)\n",
    "\n",
    "# Generate a folder per each iteration\n",
    "number_of_iterations = len(raster_list) #  - 1\n",
    "iteration_folders = [illogical_path]\n",
    "for iteration in range(1, number_of_iterations, 1):\n",
    "    iteration_ouput = os.path.join(output_tmp_path, f\"iteration_files_{iteration}\")\n",
    "    create_folder_if_not_exists(iteration_ouput)\n",
    "    iteration_folders.append(iteration_ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wroking in folder: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\illogical_files\n",
      "Wroking in folder: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\iteration_files_1\n",
      "Comparing pixels at rows/columns (13312,9472) from (13385, 9670)\r"
     ]
    }
   ],
   "source": [
    "\"\"\"Do the comparison by each level\"\"\"\n",
    "for folder in range(len(iteration_folders) - 1): # We don't want to iterate the last one.\n",
    "    current_folder = iteration_folders[folder]\n",
    "    next_folder = iteration_folders[folder + 1]\n",
    "   \n",
    "    print(f\"Working in folder: {current_folder}\")\n",
    "    raster_list = get_raster_file_list(current_folder)\n",
    "\n",
    "    all_pairs_path_list = generate_consecutive_pairs(raster_list)\n",
    "    for raster_pair in all_pairs_path_list[:]:\n",
    "        compare_rasters(raster_pair, next_folder, rule_values_list = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reverse the folder list\"\"\"\n",
    "#This is done separated so we don't do re-reverse at re-executing\n",
    "iteration_folders.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the aggregated table and build it\"\"\"\n",
    "aggregated_df = process_folders(iteration_folders[:])\n",
    "aggregated_df = add_name_columns_to_dataframe(aggregated_df, names_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build the final table\"\"\"\n",
    "final_raster = get_raster_file_list(iteration_folders[0])[0] # There is only one file\n",
    "vectorize_raster(final_raster, output_path, aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>2010_1</th>\n",
       "      <th>2015_2</th>\n",
       "      <th>2015_3</th>\n",
       "      <th>2020_4</th>\n",
       "      <th>2015_5</th>\n",
       "      <th>2020_6</th>\n",
       "      <th>2020_7</th>\n",
       "      <th>2025_8</th>\n",
       "      <th>2010_1txt</th>\n",
       "      <th>2015_2txt</th>\n",
       "      <th>2015_3txt</th>\n",
       "      <th>2020_4txt</th>\n",
       "      <th>2015_5txt</th>\n",
       "      <th>2020_6txt</th>\n",
       "      <th>2020_7txt</th>\n",
       "      <th>2025_8txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aeropuertos</td>\n",
       "      <td>Arbustal abierto esclerófilo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Aeropuertos</td>\n",
       "      <td>Arbustal abierto esclerófilo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aeropuertos</td>\n",
       "      <td>Arbustal abierto esclerófilo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bosque abierto alto</td>\n",
       "      <td>Arbustal abierto mesófilo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Arenales</td>\n",
       "      <td>Bosque abierto alto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Aeropuertos</td>\n",
       "      <td>Arbustal abierto mesófilo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arbustal</td>\n",
       "      <td>Algodón</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afloramientos rocosos</td>\n",
       "      <td>Bosque abierto alto de tierra firme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aeropuerto con infraestructura asociada</td>\n",
       "      <td>Arracachal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arbustal</td>\n",
       "      <td>Ajonjolí</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>262</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Arbustal</td>\n",
       "      <td>Ajonjolí</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arbustal</td>\n",
       "      <td>Ajonjolí</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  2010_1  2015_2  2015_3  2020_4  2015_5  2020_6  2020_7  2025_8  \\\n",
       "0      1     0.0     0.0     0.0     0.0     0.0     0.0     3.0    12.0   \n",
       "1     64     3.0    12.0     0.0     0.0     0.0     0.0     3.0    12.0   \n",
       "2      2    19.0    13.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3      5    15.0    19.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4      7     3.0    13.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "..   ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "258  248     0.0     0.0     0.0     0.0     0.0     0.0    10.0     8.0   \n",
       "259  250     0.0     0.0     0.0     0.0     0.0     0.0     4.0    20.0   \n",
       "260  251     0.0     0.0     0.0     0.0     0.0     0.0     1.0    16.0   \n",
       "261  257     0.0     0.0     0.0     0.0     0.0     0.0    10.0     7.0   \n",
       "262  262    10.0     7.0     0.0     0.0     0.0     0.0    10.0     7.0   \n",
       "\n",
       "               2010_1txt                     2015_2txt 2015_3txt 2020_4txt  \\\n",
       "0                    NaN                           NaN       NaN       NaN   \n",
       "1            Aeropuertos  Arbustal abierto esclerófilo       NaN       NaN   \n",
       "2    Bosque abierto alto     Arbustal abierto mesófilo       NaN       NaN   \n",
       "3               Arenales           Bosque abierto alto       NaN       NaN   \n",
       "4            Aeropuertos     Arbustal abierto mesófilo       NaN       NaN   \n",
       "..                   ...                           ...       ...       ...   \n",
       "258                  NaN                           NaN       NaN       NaN   \n",
       "259                  NaN                           NaN       NaN       NaN   \n",
       "260                  NaN                           NaN       NaN       NaN   \n",
       "261                  NaN                           NaN       NaN       NaN   \n",
       "262             Arbustal                      Ajonjolí       NaN       NaN   \n",
       "\n",
       "    2015_5txt 2020_6txt                                2020_7txt  \\\n",
       "0         NaN       NaN                              Aeropuertos   \n",
       "1         NaN       NaN                              Aeropuertos   \n",
       "2         NaN       NaN                                      NaN   \n",
       "3         NaN       NaN                                      NaN   \n",
       "4         NaN       NaN                                      NaN   \n",
       "..        ...       ...                                      ...   \n",
       "258       NaN       NaN                                 Arbustal   \n",
       "259       NaN       NaN                    Afloramientos rocosos   \n",
       "260       NaN       NaN  Aeropuerto con infraestructura asociada   \n",
       "261       NaN       NaN                                 Arbustal   \n",
       "262       NaN       NaN                                 Arbustal   \n",
       "\n",
       "                               2025_8txt  \n",
       "0           Arbustal abierto esclerófilo  \n",
       "1           Arbustal abierto esclerófilo  \n",
       "2                                    NaN  \n",
       "3                                    NaN  \n",
       "4                                    NaN  \n",
       "..                                   ...  \n",
       "258                              Algodón  \n",
       "259  Bosque abierto alto de tierra firme  \n",
       "260                           Arracachal  \n",
       "261                             Ajonjolí  \n",
       "262                             Ajonjolí  \n",
       "\n",
       "[263 rows x 17 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_forge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
