{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illogical transitions as a series\n",
    "- This script gets a batch of vector files located in a folder and creates a single output wrapping all the series identifying where and what illogical transitions happen in all the AOI.\n",
    "\n",
    "- Inputs\n",
    "    - Path of the folder with the vector files.\n",
    "    - Path of the csv with all the illogical transitions (val1, val2) without headers.\n",
    "    - Column reference name of the vector files for rasterization.\n",
    "    - Path of the csv file without headers containing or:\n",
    "        - 1 column table with all the accurate names.\n",
    "        - 2 column table with the values and its corresponding names.\n",
    "- Outputs\n",
    "    - Path of the output folder with the final vector file.\n",
    "    - Path of all the intermediate processing files.\n",
    "- Processing\n",
    "    - Read all the vector input files.\n",
    "    - Optional: Dissolve them to make the rest of the process easier.\n",
    "    - Check the input column if it is based on values or text.\n",
    "    - Create the corresponding dictionary for classification.\n",
    "    - Homogenize the reference column with the dictionary.\n",
    "    - Create intermediate vector files.\n",
    "    - Rasterize the reference column.\n",
    "    - Compare all the years in pairs creating the corresponding illogical files.\n",
    "        - Raster files with ID values.\n",
    "        - csv with ID values and pair accumulated values.\n",
    "    - Create a folder system with all the required iterations.\n",
    "    - Compare all the files inside the folder system.\n",
    "    - Get all the generated csvs.\n",
    "    - Wrap up all the info in a single table.\n",
    "    - Append the text info to the table.\n",
    "    - Vectorize the final output.\n",
    "- Notes:\n",
    "    - This is a variation with dask for testing\n",
    "- Author\n",
    "    - RubÃ©n Crespo Ceballos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, ogr\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import csv\n",
    "import dask.array as da\n",
    "import dask_geopandas as dgpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_if_not_exists(folder_path):\n",
    "    \"\"\"\n",
    "    Create a folder if it doesn't exist.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): The path of the folder to be created.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder created at: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Folder already exists at: {folder_path}\")\n",
    "\n",
    "def get_vector_file_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of the vector files inside the folder\n",
    "    Parameters:\n",
    "    - path (str): path of the folder with the resources.\n",
    "\n",
    "    Returns:\n",
    "    - File_list (list). list of the resources.\n",
    "    \"\"\"\n",
    "    File_list = [] #f for f in os.listdir(path) if os.isfile(mypath,f)\n",
    "    for file in os.listdir(path):\n",
    "        # \"anat\" is just to get here necessary ones\n",
    "        if file.endswith(\".shp\"):\n",
    "            if file not in File_list:\n",
    "                File_list.append(os.path.join(path,file))\n",
    "        else:\n",
    "            pass\n",
    "    return File_list\n",
    "\n",
    "def get_raster_file_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of the raster files inside the folder\n",
    "    Parameters:\n",
    "    - path (str): path of the folder with the resources.\n",
    "\n",
    "    Returns:\n",
    "    - File_list (list). list of the resources.\n",
    "    \"\"\"\n",
    "    File_list = [] #f for f in os.listdir(path) if os.isfile(mypath,f)\n",
    "    for file in os.listdir(path):\n",
    "        # \"32628\" is just to get here necessary ones\n",
    "        if file.endswith(\".tif\") or file.endswith(\".tiff\"):\n",
    "            if file not in File_list:\n",
    "                File_list.append(os.path.join(path,file))\n",
    "        else:\n",
    "            pass\n",
    "    return File_list\n",
    "\n",
    "def get_csv_file_list(path):\n",
    "    \"\"\"\n",
    "    Get a list of the csv files inside the folder\n",
    "    Parameters:\n",
    "    - path (str): path of the folder with the resources.\n",
    "\n",
    "    Returns:\n",
    "    - File_list (list). list of the resources.\n",
    "    \"\"\"\n",
    "    File_list = [] #f for f in os.listdir(path) if os.isfile(mypath,f)\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            if file not in File_list:\n",
    "                File_list.append(os.path.join(path,file))\n",
    "        else:\n",
    "            pass\n",
    "    return File_list\n",
    "\n",
    "def find_best_match(name_gdf, unique_names, similarity_threshold):\n",
    "    \"\"\"\n",
    "    Find the best match for a given name using SequenceMatcher.\n",
    "\n",
    "    Parameters:\n",
    "    - name_gdf (str): The name to be compared.\n",
    "    - unique_names (list): List of unique names.\n",
    "    - similarity_threshold (float): Minimum similarity ratio to accept.\n",
    "\n",
    "    Returns:\n",
    "    - (str, float): Best matching name and its similarity score.\n",
    "    \"\"\"\n",
    "    highest_similarity_ratio = 0\n",
    "    best_matching_name = None\n",
    "\n",
    "    for unique_name in unique_names:\n",
    "        similarity_ratio = SequenceMatcher(None, unique_name, name_gdf).ratio()\n",
    "        if similarity_ratio > highest_similarity_ratio:\n",
    "            highest_similarity_ratio = similarity_ratio\n",
    "            best_matching_name = unique_name\n",
    "\n",
    "    return best_matching_name if highest_similarity_ratio >= similarity_threshold else None, highest_similarity_ratio\n",
    "\n",
    "def update_names_based_on_similarity(unique_names, ddf, column_name, similarity_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Dask-compatible function to update names in a GeoDataFrame based on similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - unique_names (list): List of unique names.\n",
    "    - ddf (Dask GeoDataFrame): The Dask GeoDataFrame whose names need to be updated.\n",
    "    - column_name (str): Column name containing names.\n",
    "    - similarity_threshold (float): Threshold for similarity ratio.\n",
    "\n",
    "    Returns:\n",
    "    - Updated Dask GeoDataFrame.\n",
    "    \"\"\"\n",
    "    # Apply the matching function to each row\n",
    "    ddf[['valid_text', 'simil_val']] = ddf[column_name].apply(\n",
    "        lambda x: find_best_match(x, unique_names, similarity_threshold),\n",
    "        meta={'valid_text': 'str', 'simil_val': 'float64'}  # Metadata for Dask\n",
    "    )\n",
    "\n",
    "    return ddf\n",
    "\n",
    "\n",
    "def create_identifier_dictionary(list):\n",
    "    \"\"\"\n",
    "    Creates a dictionary out of a list appending a new id to each one of them.\n",
    "\n",
    "    Parameters:\n",
    "    - list (list): list of strings.\n",
    "\n",
    "    Returns:\n",
    "    - value_to_text_dict. dictionary o value: text.\n",
    "    \"\"\"\n",
    "    value_to_text_dict = {index + 1: value for index, value in enumerate(sorted(list))} # {val: text}\n",
    "    return value_to_text_dict\n",
    "\n",
    "\n",
    "\n",
    "def gdal_rasterize_from_shapefile(shapefile_path, resolution, nodata_value, data_type, output_path, cols=None, rows=None):\n",
    "    \"\"\"\n",
    "    Rasterizes a GeoDataFrame using GDAL directly.\n",
    "\n",
    "    Parameters:\n",
    "    - shapefile_path (string): path of the vector file.\n",
    "    - resolution (int or float): Resolution of the raster (pixel size).\n",
    "    - nodata_value: The value to use for no-data pixels.\n",
    "    - data_type: Data type for the output raster (e.g., gdal.GDT_Float32).\n",
    "    - output_path (str): Path to save the output raster.\n",
    "    - cols (int, optional): Number of columns in the output raster.\n",
    "    - rows (int, optional): Number of rows in the output raster.\n",
    "\n",
    "    Returns:\n",
    "    - None. The function writes the raster to the specified output path.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the Shapefile using OGR\n",
    "    shapefile = ogr.Open(shapefile_path)\n",
    "    layer = shapefile.GetLayer()\n",
    "\n",
    "    # Get the bounds of the Shapefile (same as GeoDataFrame's total_bounds)\n",
    "    xmin, xmax, ymin, ymax = layer.GetExtent()\n",
    "\n",
    "    # If cols and rows are not provided, calculate them based on resolution\n",
    "    if cols is None or rows is None:\n",
    "        cols = int((xmax - xmin) / resolution)\n",
    "        rows = int((ymax - ymin) / resolution)\n",
    "\n",
    "    # Create a new raster dataset\n",
    "    raster_ds = gdal.GetDriverByName('GTiff').Create(\n",
    "        output_path, cols, rows, 1, data_type,\n",
    "        options=['COMPRESS=DEFLATE', 'TILED=YES']\n",
    "    )\n",
    "\n",
    "    # Set the geotransform (affine transform for the raster)\n",
    "    geotransform = (xmin, resolution, 0, ymax, 0, -resolution)\n",
    "    raster_ds.SetGeoTransform(geotransform)\n",
    "\n",
    "    # Set the CRS (coordinate reference system) from the Shapefile\n",
    "    srs = layer.GetSpatialRef()\n",
    "    if srs:\n",
    "        raster_ds.SetProjection(srs.ExportToWkt())\n",
    "\n",
    "    # Create the raster band and set no-data value\n",
    "    band = raster_ds.GetRasterBand(1)\n",
    "    band.SetNoDataValue(nodata_value)\n",
    "\n",
    "    # Rasterize the shapefile\n",
    "    gdal.RasterizeLayer(\n",
    "        raster_ds,  # Output raster dataset\n",
    "        [1],        # Raster band to write to\n",
    "        layer,      # Input OGR layer to rasterize\n",
    "        options=['ATTRIBUTE=raster_val', 'ALL_TOUCHED=TRUE']\n",
    "    )\n",
    "\n",
    "    # Flush and close the raster dataset\n",
    "    band.FlushCache()\n",
    "    raster_ds = None  # Close the file and save\n",
    "\n",
    "    # Close the shapefile\n",
    "    shapefile = None\n",
    "\n",
    "    print(f\"Rasterization complete: {output_path}\")\n",
    "\n",
    "    return cols, rows\n",
    "\n",
    "def check_same_dimensions(raster_files):\n",
    "    \"\"\"\n",
    "    Check the if the dimensions of all the input rasters have the same dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_files (list): List of raster files.\n",
    "\n",
    "    Returns:\n",
    "    - dimensions_list (list). The list with all the dimensions.\n",
    "    \"\"\"\n",
    "    dimensions_list = []\n",
    "\n",
    "    # Open the first raster file in the list\n",
    "    for file_path in raster_files[:]:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            shape_dimensions = src.shape\n",
    "            dimensions_list.append(shape_dimensions)\n",
    "\n",
    "    if len(set(dimensions_list)) == 1:\n",
    "        print(f\"All the elements have the same dimensions{dimensions_list[0]}\")\n",
    "        dimensions_list\n",
    "    else:\n",
    "        print(\"The dimensions are note the same\")\n",
    "        return dimensions_list\n",
    "\n",
    "\n",
    "def read_csv_in_pairs(csv_file):\n",
    "    \"\"\"\n",
    "    Reads a two column csv and transforms it into a pair value list.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file (str): path of the csv file.\n",
    "\n",
    "    Returns:\n",
    "    - rule_values_list: list of unique value pairs.\n",
    "    \"\"\"\n",
    "    rule_values_list = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        # next(file)  # Skip the header row\n",
    "        for line in file:\n",
    "            # Split the line into two values based on spaces, and convert them to floats\n",
    "            rule_value_1, rule_value_2 = map(float, line.split(\",\"))\n",
    "            rule_values_list.append((rule_value_1, rule_value_2))\n",
    "    return rule_values_list\n",
    "\n",
    "def csv_to_dict(file_path):\n",
    "    \"\"\"\n",
    "    Creates a dictionary out of a csv of two columns excluding the header.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (string): path of the file.\n",
    "\n",
    "    Returns:\n",
    "    - result_dict(dict). dictionary o value: text.\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "\n",
    "    # Open the CSV file\n",
    "    with open(file_path, mode='r', newline='', encoding='ISO-8859-1') as file: # encoding='ISO-8859-1' encoding='utf-8'\n",
    "        reader = csv.reader(file)\n",
    "\n",
    "        # Skip the header if it has it\n",
    "        next(reader)\n",
    "\n",
    "        # Iterate through each row and add to the dictionary\n",
    "        for row in reader:\n",
    "            \n",
    "            key = row[0]  # First column as the key\n",
    "            value = row[1]  # Second column as the value\n",
    "            result_dict[float(key)] = value\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def csv_to_list(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with one column and transforms it into a list.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (string): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - return(list): List containing the values from the column\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    with open(file_path, mode='r', newline='', encoding='ISO-8859-1') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row:  # Ensure the row is not empty\n",
    "                result.append(row[0])\n",
    "    return result\n",
    "\n",
    "def generate_consecutive_pairs(paths_list):\n",
    "    \"\"\"\n",
    "    Generates a list of consecutive pairs from the input list in order.\n",
    "\n",
    "    Parameters:\n",
    "    - paths_list (list): list of paths.\n",
    "\n",
    "    Returns:\n",
    "    - consecutive_pairs: list of consecutive value pairs.\n",
    "    \"\"\"\n",
    "    # Create consecutive pairs using zip\n",
    "    consecutive_pairs = list(zip(paths_list, paths_list[1:]))\n",
    "    return consecutive_pairs\n",
    "\n",
    "def compare_rasters_dask(raster_pair, output, rule_values_list=None):\n",
    "    \"\"\"\n",
    "    Compares two raster files using Dask for parallel processing, applies rules if provided,\n",
    "    and generates an output raster and comparison dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_pair (list): List of two file paths to the raster files to compare.\n",
    "    - output (str): Path to the output directory.\n",
    "    - rule_values_list (list or None): List of value pairs representing illogical transitions.\n",
    "\n",
    "    Returns:\n",
    "    - output_loc (str): Path to the output raster file.\n",
    "    - comparison_df: Pandas DataFrame of unique value pairs with corresponding IDs.\n",
    "    \"\"\"\n",
    "    # Open raster files\n",
    "    ds1 = gdal.Open(raster_pair[0])\n",
    "    ds2 = gdal.Open(raster_pair[1])\n",
    "\n",
    "    if not ds1 or not ds2:\n",
    "        raise ValueError(\"Error: Unable to open raster files.\")\n",
    "\n",
    "    # Ensure both rasters have the same dimensions\n",
    "    if ds1.RasterXSize != ds2.RasterXSize or ds1.RasterYSize != ds2.RasterYSize:\n",
    "        raise ValueError(\"Error: Rasters have different dimensions.\")\n",
    "\n",
    "    width = ds1.RasterXSize\n",
    "    height = ds1.RasterYSize\n",
    "    geotransform = ds1.GetGeoTransform()\n",
    "    projection = ds1.GetProjection()\n",
    "\n",
    "    # Define output raster details\n",
    "    layer_1 = os.path.basename(raster_pair[0]).split('_')[-1].replace(\".tif\", \"\")\n",
    "    layer_2 = os.path.basename(raster_pair[1]).split('_')[-1].replace(\".tif\", \"\")\n",
    "    output_filename = f\"illogical_transitions_{layer_1}-{layer_2}.tif\"\n",
    "    output_loc = os.path.join(output, output_filename)\n",
    "\n",
    "    # Create a Dask array for each raster\n",
    "    block_size = 256  # Define chunk size\n",
    "    dask_array1 = da.from_array(ds1.GetRasterBand(1).ReadAsArray(), chunks=(block_size, block_size))\n",
    "    dask_array2 = da.from_array(ds2.GetRasterBand(1).ReadAsArray(), chunks=(block_size, block_size))\n",
    "\n",
    "    # Convert rule_values_list to a set for faster lookup\n",
    "    rule_values_set = set(tuple(pair) for pair in rule_values_list) if rule_values_list else None\n",
    "\n",
    "    # Define the processing function for each block\n",
    "    def process_block(block1, block2, rule_values_set):\n",
    "        output_block = np.zeros(block1.shape, dtype=np.int16) # I don't expect it to be bigger\n",
    "        unique_value_dict = {} # pairs : unique_value\n",
    "\n",
    "        for i in range(block1.shape[0]):\n",
    "            for j in range(block1.shape[1]):\n",
    "                value1 = block1[i, j]\n",
    "                value2 = block2[i, j]\n",
    "\n",
    "                if rule_values_set:  # Check against the rule values\n",
    "                    if (value1, value2) in rule_values_set:\n",
    "                        unique_value = unique_value_dict.setdefault((value1, value2), len(unique_value_dict) + 1)\n",
    "                        \n",
    "                        output_block[i, j] = unique_value\n",
    "                    else:\n",
    "                        output_block[i, j] = 0 # Assign 0 if the pair is not in the rule set\n",
    "                else:  # Handle all combinations if no rules are provided\n",
    "                    if value1 or value2:\n",
    "                        unique_value = unique_value_dict.setdefault((value1, value2), len(unique_value_dict) + 1)\n",
    "                        output_block[i, j] = unique_value\n",
    "\n",
    "        return output_block, unique_value_dict\n",
    "\n",
    "    # Apply the processing function across all blocks of the array\n",
    "    # It calls the function strange, but it works like\n",
    "    result = da.map_blocks(process_block, dask_array1, dask_array2, rule_values_set, dtype=np.int16)\n",
    "\n",
    "    # Compute the output array\n",
    "    output_array = result.compute() # It turns the dask array into a regular one.\n",
    "\n",
    "    # Save the output raster\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    output_ds = driver.Create(output_loc, width, height, 1, gdal.GDT_Int16, options=['COMPRESS=DEFLATE', 'TILED=YES'])\n",
    "    output_ds.GetRasterBand(1).WriteArray(output_array)\n",
    "    output_ds.GetRasterBand(1).SetNoDataValue(0)\n",
    "    output_ds.SetGeoTransform(geotransform)\n",
    "    output_ds.SetProjection(projection)\n",
    "    output_ds = None  # Close the dataset\n",
    "\n",
    "    # Create the comparison DataFrame\n",
    "    unique_values = np.unique(output_array[output_array != 0])\n",
    "    unique_value_dict = {}  # Collect unique pairs from the computation\n",
    "    value_pairs = [k for k, v in unique_value_dict.items() if v in unique_values]\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'unival': [unique_value_dict[pair] for pair in value_pairs],\n",
    "        f'{layer_1}': [pair[0] for pair in value_pairs],\n",
    "        f'{layer_2}': [pair[1] for pair in value_pairs]\n",
    "    })\n",
    "\n",
    "    # Save the comparison DataFrame to a CSV file\n",
    "    comparison_df.to_csv(os.path.join(output, f\"illo_tab_{layer_1}-{layer_2}.csv\"), index=False)\n",
    "\n",
    "    return print(f\"Finished processing: {output_loc}\")\n",
    "\n",
    "def vectorize_raster(raster_path, output_path, df):\n",
    "    \"\"\"\n",
    "    Vectorizes an input raster.\n",
    "\n",
    "    Parameters:\n",
    "    - raster_path (str): path of the input raster.\n",
    "    - output_path (path): path of the output file.\n",
    "    - df (dataframe): dataframe related to the input raster.\n",
    "\n",
    "    Returns:\n",
    "    - None. It produces the vector file.\n",
    "    \"\"\"\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Read raster data as numpy array\n",
    "        data = src.read(1)\n",
    "        # Get affine transform of the raster\n",
    "        transform = src.transform\n",
    "\n",
    "    # Vectorize the raster data\n",
    "    raster_shapes = features.shapes(data, transform=transform)\n",
    "\n",
    "    # Convert the vectorized shapes into a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame.from_features(\n",
    "        [\n",
    "            {\"geometry\": geo_shape, \"properties\": {\"ID\": value}}\n",
    "            for geo_shape, value in raster_shapes\n",
    "        ],\n",
    "        crs=src.crs\n",
    "    )\n",
    "\n",
    "    # Merge the dataframe with the GeoDataFrame\n",
    "    merged_gdf = gdf.merge(df, on='ID')\n",
    "    # Dissolve based on a column value\n",
    "    dissolved_gdf = merged_gdf.dissolve(by='ID')\n",
    "\n",
    "    # Save the merged GeoDataFrame as a new shapefile\n",
    "    filename = os.path.join(output_path, \"illogical_transitions\")\n",
    "    dissolved_gdf.to_file(f'{filename}.shp', driver='ESRI Shapefile')\n",
    "    return\n",
    "\n",
    "def add_name_columns_to_dataframe(df, names_dictionary):\n",
    "    \"\"\"\n",
    "    Adds the input name list into the  an input raster.\n",
    "\n",
    "    Parameters:\n",
    "    - df (dataframe): dataframe related to the input raster.\n",
    "    - names_dictionary (dict): dict of the names that we want to append.\n",
    "\n",
    "    Returns:\n",
    "    - df (dataframe). It produces the updated dataframe with the names.\n",
    "    \"\"\"\n",
    "    # Get the original columns (without including any new columns)\n",
    "    original_columns = df.columns.tolist()\n",
    "\n",
    "    for column in original_columns[1:]: # Skip the first column\n",
    "        df[column + \"txt\"] = df[column].map(names_dictionary)\n",
    "        # Alternative\n",
    "        # column_list = column.split(\"_\") # Example 2019_1\n",
    "        # df[column_list[0] + \"_text_\" + column_list[1]] = df[column].map(names_dictionary)\n",
    "\n",
    "    # Sort the columns\n",
    "    # order_columns = [df.columns[0]] + sorted(df.columns[1:])\n",
    "    # df = df[order_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "def dissolve_geodataframe(gdf, column):\n",
    "    \"\"\"\n",
    "    Dissolves a GeoDataFrame based on unique values of a specified column.\n",
    "\n",
    "    Parameters:\n",
    "    gdf (GeoDataFrame): Input GeoDataFrame to be dissolved.\n",
    "    column (str): Column name based on which to dissolve the GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: The dissolved GeoDataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Initial number of geometries: {len(gdf)}\")\n",
    "\n",
    "    # Perform the dissolve operation\n",
    "    dissolved_gdf = gdf.dissolve(by=column)\n",
    "\n",
    "    print(f\"Final number of geometries after dissolve: {len(dissolved_gdf)}\")\n",
    "\n",
    "    return dissolved_gdf\n",
    "\n",
    "def process_folders(iteration_folders, idx=0, df=None, aggregated_df=None):\n",
    "    \"\"\"\n",
    "    Recursively processes a series of folders containing CSV files to merge data based on shared columns.\n",
    "\n",
    "    Parameters:\n",
    "    iteration_folders (list): A list of folder paths to iterate through, each containing CSV files.\n",
    "    idx (int): The current folder index being processed. Defaults to 0.\n",
    "    df (DataFrame): The current DataFrame being merged. Defaults to None for the first iteration.\n",
    "    aggregated_df (DataFrame): The cumulative aggregated DataFrame. Defaults to None for the first iteration.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The final aggregated DataFrame containing merged data from all folders.\n",
    "\n",
    "    Functionality:\n",
    "    - Reads the initial CSV from the first folder and initializes the aggregation process.\n",
    "    - Iterates through columns in the current DataFrame (`df`), identifying corresponding CSV files in the next folder.\n",
    "    - Merges data from matching files into the current DataFrame using inner joins, while retaining unmatched rows with outer joins.\n",
    "    - Adds unique, incrementing suffixes to column names to avoid duplicates.\n",
    "    - Recursively processes subsequent folders, updating `aggregated_df` with new merged data at each step.\n",
    "    - Ensures columns are reordered to keep 'ID' first and all other columns sorted alphabetically.\n",
    "    - Stops when all folders are processed.\n",
    "    \"\"\"\n",
    "    if idx >= len(iteration_folders) - 1:\n",
    "        return aggregated_df\n",
    "\n",
    "    current_folder = iteration_folders[idx]\n",
    "    next_folder = iteration_folders[idx + 1]\n",
    "\n",
    "    if df is None and aggregated_df is None:\n",
    "        # Read the initial csv\n",
    "        csv_files = [f for f in os.listdir(current_folder) if f.endswith('.csv')]\n",
    "        file_path = os.path.join(current_folder, csv_files[0])\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Create the aggregated_df\n",
    "        df = df.rename(columns={'unival': 'ID'})\n",
    "        unival_series = df['ID']\n",
    "        aggregated_df = pd.DataFrame(unival_series.to_frame())\n",
    "\n",
    "    column_names = df.columns[1:].tolist() # we dont want the first column\n",
    "\n",
    "    counter = 1  # Restart the counter at 1 for each folder iteration\n",
    "\n",
    "    for column_name in column_names:\n",
    "        # The next iterations names come with a suffix. Remove it for searching\n",
    "        if \"_\" in column_name:\n",
    "            base_column_name = column_name.split(\"_\")[0]\n",
    "        else:\n",
    "            base_column_name = column_name\n",
    "\n",
    "        # Look for the matching CSV in the next folder, search and open the csv\n",
    "        csv_files = [f for f in os.listdir(next_folder) if f.endswith('.csv')]\n",
    "        matching_csv = [file for file in csv_files if file.startswith(base_column_name)][0] # we expect only one element\n",
    "        if not matching_csv:\n",
    "            raise FileNotFoundError(f\"No matching file found for {base_column_name} in {next_folder}\")\n",
    "        matching_path = os.path.join(next_folder, matching_csv)\n",
    "        match_df = pd.read_csv(matching_path)\n",
    "\n",
    "        # Add a suffix to the columns so we can diferenciate then. Increment by the number of columns added to match_df (excluding the first column)\n",
    "        match_df.columns = [match_df.columns[0]] + [f'{col}_{counter}' for counter, col in enumerate(match_df.columns[1:], start=counter)]\n",
    "        counter += len(match_df.columns) - 1\n",
    "\n",
    "        # Merge the data (internal)\n",
    "        df = df.rename(columns={column_name: base_column_name}) # Rename the column to do the merge.\n",
    "        merged_df = pd.merge(df, match_df, left_on=base_column_name, right_on=\"unival\", how='outer') #outer means that we keep all the combinations\n",
    "\n",
    "        # THIS IS OPTIONAL: Fill NAN to 0.\n",
    "        merged_df = merged_df.fillna(0)\n",
    "\n",
    "        # To surpass duplicate names\n",
    "        merged_df = merged_df.drop(['unival', base_column_name], axis=1)\n",
    "        merged_df = merged_df.drop(columns=column_names, errors='ignore')\n",
    "\n",
    "        df = df.rename(columns={base_column_name: column_name}) # We put the name back\n",
    "\n",
    "        # Merge with aggregated_df (external)\n",
    "        aggregated_df = pd.merge(merged_df, aggregated_df, on='ID', how='inner')\n",
    "\n",
    "    # Drop columns that were used for merging in the current iteration\n",
    "    aggregated_df = aggregated_df.drop(columns=column_names, errors='ignore')\n",
    "\n",
    "    # Reorder columns after the first column\n",
    "    order_columns = [aggregated_df.columns[0]] + sorted(aggregated_df.columns[1:], key=lambda x: int(x.split('_')[1])) # Remove after \", key=...\" if any issue arisses\n",
    "    aggregated_df = aggregated_df[order_columns]\n",
    "    return process_folders(iteration_folders, idx=idx + 1, df=aggregated_df, aggregated_df=aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: Z:\\z_resources\\ruben\\landcover_vector_files_for_illogical_testing\\output_tmp_path\n",
      "Folder already exists at: Z:\\z_resources\\ruben\\landcover_vector_files_for_illogical_testing\\output_path\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Specify all the inputs\"\"\"\n",
    "# Path for the vector inputs\n",
    "input_path = r\"Z:\\z_resources\\ruben\\landcover_vector_files_for_illogical_testing\"\n",
    "\n",
    "# Path for the final output\n",
    "output_path = r\"Z:\\z_resources\\ruben\\landcover_vector_files_for_illogical_testing\\output_path\"\n",
    "\n",
    "# Path of the illogical rules csv\n",
    "rule_table_path = r\"Z:\\z_resources\\ruben\\landcover_vector_files_for_illogical_testing\\illogical_transitions\\output_files\\illogical_transitions.csv\"\n",
    "\n",
    "#Path of all the intermediate outputs (Don't touch this)\n",
    "output_tmp_path = input_path + r\"\\output_tmp_path\"\n",
    "\n",
    "#Create the paths\n",
    "create_folder_if_not_exists(output_tmp_path)\n",
    "create_folder_if_not_exists(output_path)\n",
    "\n",
    "\"\"\"We have here two situations. Comment the non necessary one\"\"\"\n",
    "\n",
    "# If we have only strings with no values. We append a created value.\n",
    "# names_list_path = r\"Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\names_list.csv\"\n",
    "# names_list = csv_to_list(names_list_path)\n",
    "# names_dictionary = create_identifier_dictionary(names_list) # The output will always be number: text \n",
    "\n",
    "# If we have both strings and values\n",
    "names_dictionary_path = r\"Z:\\z_resources\\ruben\\landcover_vector_files_for_illogical_testing\\names_dictionary.csv\" #It has headers\n",
    "names_dictionary = csv_to_dict(names_dictionary_path) # {Code: label}\n",
    "names_list = list(names_dictionary.values())\n",
    "\n",
    "vector_file_list = get_vector_file_list(input_path)\n",
    "\n",
    "\n",
    "\"\"\"For rasterization\"\"\"\n",
    "# Specify the column to rasterize by.\n",
    "column_name = 'leyenda'\n",
    "# Define the resolution of your raster.\n",
    "resolution = 30  # in meters\n",
    "# Define the nodata value of your raster.\n",
    "nodata_value = 0\n",
    "# Define the data type of the raster.\n",
    "data_type = gdal.GDT_UInt16\n",
    "\"\"\"\n",
    "gdal.GDT_Byte,\n",
    "gdal.GDT_Int16,\n",
    "gdal.GDT_UInt16,\n",
    "gdal.GDT_Int32,\n",
    "gdal.GDT_UInt32,\n",
    "gdal.GDT_Float32,\n",
    "gdal.GDT_Float64\n",
    "\"\"\"\n",
    "# Define the rows and columns for the rasterization (All the files must have the same dimensions).\n",
    "# You can put the parameters here manually, or let the system take the reference of the first rasterization.\n",
    "rows = None\n",
    "columns = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"OPTIONAL: For heavy vector files, we are going to dissolve based on the designed column, to simplify the rest of the process\"\"\"\n",
    "dissolved_files_path = output_tmp_path + r\"\\dissolved_files\"\n",
    "create_folder_if_not_exists(dissolved_files_path)\n",
    "\n",
    "for file in vector_file_list[:]:\n",
    "    gdf = gpd.read_file(file)\n",
    "    print(\"gdf opened\")\n",
    "    gdf = dissolve_geodataframe(gdf, column_name)\n",
    "    gdf.to_file(os.path.join(dissolved_files_path, os.path.basename(file).replace(\".shp\", \"_dissolved.shp\")) , driver='ESRI Shapefile')\n",
    "    \n",
    "# Read the new imputs\n",
    "vector_file_list = get_vector_file_list(dissolved_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: Z:\\z_resources\\ruben\\landcover_vector_files_for_illogical_testing\\output_tmp_path\\optimized_vector_files\n",
      "gdf cobertura_tierra_2020_epsg3316_100k_2020_dissolved_optimized.shp opened\n",
      "The column 'leyenda' contains strings.\n",
      "names updated\n",
      "gdf cobertura_tierra_clc_2011(from_mec_2011)_3116_dissolved_optimized.shp opened\n",
      "The column 'leyenda' contains strings.\n",
      "names updated\n",
      "gdf cobertura_tierra_clc_2018_3116_dissolved_optimized.shp opened\n",
      "The column 'leyenda' contains strings.\n",
      "names updated\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create the \"raster_val\" for rasterization according to the content of the columnn\"\"\"\n",
    "optimized_vector_files_path = output_tmp_path + r\"\\optimized_vector_files\"\n",
    "create_folder_if_not_exists(optimized_vector_files_path)\n",
    "\n",
    "# Create a list to store filtered DataFrames\n",
    "filtered_dfs = []\n",
    "\n",
    "for file in vector_file_list[:]:\n",
    "    dgdf = dgpd.read_file(file, npartitions=4) # We split it into this paralelism, a partition per core.\n",
    "    print(f\"gdf {os.path.basename(file)} opened\")\n",
    "    if dgdf[column_name].dtype == object:\n",
    "        print(f\"The column '{column_name}' contains strings.\")\n",
    "        dgdf = update_names_based_on_similarity(names_list, dgdf, column_name, similarity_threshold=0.3)\n",
    "        print(\"names updated\")\n",
    "        # Add a new column to the GeoDataFrame containing the unique identifiers\n",
    "        dgdf['raster_val'] = dgdf[\"valid_text\"].map(names_dictionary)\n",
    "        print(\"names appended\")\n",
    "        \"\"\"Wrap the info into a df.\"\"\" # ACHTUNG: Pending to test\n",
    "        ddf = dgdf.drop(columns=[\"geometry\"])\n",
    "\n",
    "        # Use Dask for grouping instead of a for-loop\n",
    "        merged_df = ddf.groupby(\"valid_text\").apply(lambda x: x, meta=ddf)\n",
    "\n",
    "    else:\n",
    "        print(f\"The column '{column_name}' contains values.\")\n",
    "        # Get unique values/strings from the specified column, they are always sorted.\n",
    "        unique_values = sorted(dgdf[column_name].unique())\n",
    "        # Create a dictionary one to one\n",
    "        value_to_index = {value: value for value in unique_values}\n",
    "        dgdf['raster_val'] = dgdf[column_name].map(value_to_index)\n",
    "    \n",
    "    # Opening and saving the file takes a lot of time\n",
    "    # compute functions transform back the gdf to geopandas\n",
    "    # gdf = dgdf.compute().to_file(os.path.join(optimized_vector_files_path, os.path.basename(file).replace(\".shp\", \"_optimized.shp\")) , driver='ESRI Shapefile')\n",
    "\n",
    "    # Saving the csv.\n",
    "#     merged_df.to_csv(os.path.join(optimized_vector_files_path, 'optimited.csv'), index=False) \n",
    "    \n",
    "# # Read the new imputs\n",
    "# vector_file_list = get_vector_file_list(optimized_vector_files_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask-GeoPandas GeoDataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leyenda</th>\n",
       "      <th>codigo</th>\n",
       "      <th>insumo</th>\n",
       "      <th>apoyo</th>\n",
       "      <th>confiabili</th>\n",
       "      <th>cambio</th>\n",
       "      <th>nivel_1</th>\n",
       "      <th>nivel_2</th>\n",
       "      <th>nivel_3</th>\n",
       "      <th>nivel_4</th>\n",
       "      <th>nivel_5</th>\n",
       "      <th>nivel_6</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>valid_text</th>\n",
       "      <th>raster_val</th>\n",
       "      <th>geometry</th>\n",
       "      <th>simil_val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=4</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>geometry</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: assign, 10 graph layers</div>"
      ],
      "text/plain": [
       "Dask GeoDataFrame Structure:\n",
       "              leyenda codigo  insumo   apoyo confiabili  cambio nivel_1 nivel_2 nivel_3 nivel_4 nivel_5 nivel_6 Shape_Leng Shape_Area valid_text raster_val  geometry simil_val\n",
       "npartitions=4                                                                                                                                                                  \n",
       "0              object  int64  object  object     object  object  object  object  object  object  object  object    float64    float64     object     object  geometry   float64\n",
       "33                ...    ...     ...     ...        ...     ...     ...     ...     ...     ...     ...     ...        ...        ...        ...        ...       ...       ...\n",
       "66                ...    ...     ...     ...        ...     ...     ...     ...     ...     ...     ...     ...        ...        ...        ...        ...       ...       ...\n",
       "99                ...    ...     ...     ...        ...     ...     ...     ...     ...     ...     ...     ...        ...        ...        ...        ...       ...       ...\n",
       "129               ...    ...     ...     ...        ...     ...     ...     ...     ...     ...     ...     ...        ...        ...        ...        ...       ...       ...\n",
       "Dask Name: assign, 10 graph layers"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gdf = gdf.compute()\n",
    "dgdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:\\z_resources\\ruben\\landcover_vector_files_copy\\output_files\\cobertura_tierra_2020_epsg3316_100k_2020_dissolved_optimized.tif\n",
      "Rasterization complete: Y:\\z_resources\\ruben\\landcover_vector_files_copy\\output_files\\cobertura_tierra_2020_epsg3316_100k_2020_dissolved_optimized.tif\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Transform the vector files and convert them into rasters\"\"\"\n",
    "rasterized_files_path = output_tmp_path + r\"\\rasterized_files_path\"\n",
    "create_folder_if_not_exists(rasterized_files_path)\n",
    "\n",
    "for file in vector_file_list[:]:\n",
    "    output_path_file = os.path.join(rasterized_files_path, os.path.basename(file).replace(\".shp\", \".tif\"))\n",
    "    print(f\"{os.path.basename(file)} opened\")\n",
    "    print(\"Creating\" , output_path_file)\n",
    "    cols, rows = gdal_rasterize_from_shapefile(file, resolution, nodata_value, data_type, output_path_file, cols=None, rows=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the elements have the same dimensions(9670, 13385)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check the dimensions of the rasters are all the same\"\"\"\n",
    "raster_list = get_raster_file_list(rasterized_files_path)\n",
    "check_same_dimensions(raster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the illogical transitions 1 to 1\"\"\"\n",
    "\n",
    "illogical_path = output_tmp_path + r\"\\illogical_files\"\n",
    "create_folder_if_not_exists(illogical_path)\n",
    "\n",
    "raster_list = get_raster_file_list(rasterized_files_path)\n",
    "all_pairs_path_list = generate_consecutive_pairs(raster_list)\n",
    "\n",
    "# Read rule table and create a list of pairs with the infod\n",
    "rule_values_list = read_csv_in_pairs(rule_table_path)\n",
    "\n",
    "for raster_pair in all_pairs_path_list[:]:\n",
    "    compare_rasters_dask(raster_pair, illogical_path, rule_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\output_tmp_path\\iteration_files_1\n",
      "Folder already exists at: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\output_tmp_path\\iteration_files_2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create the folder system\"\"\"\n",
    "# Delte this when finishing\n",
    "illogical_path = output_tmp_path + r\"\\illogical_files\"\n",
    "raster_list = get_raster_file_list(illogical_path)\n",
    "\n",
    "# Generate a folder per each iteration\n",
    "number_of_iterations = len(raster_list) #  - 1\n",
    "iteration_folders = [illogical_path]\n",
    "for iteration in range(1, number_of_iterations, 1):\n",
    "    iteration_ouput = os.path.join(output_tmp_path, f\"iteration_files_{iteration}\")\n",
    "    create_folder_if_not_exists(iteration_ouput)\n",
    "    iteration_folders.append(iteration_ouput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wroking in folder: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\illogical_files\n",
      "Wroking in folder: Y:\\z_resources\\im-nca-senegal\\v2_shp_occsol_anat\\23-12-22\\shp_occsol_anat\\testing\\iteration_files_1\n",
      "Comparing pixels at rows/columns (13312,9472) from (13385, 9670)\r"
     ]
    }
   ],
   "source": [
    "\"\"\"Do the comparison by each level\"\"\"\n",
    "for folder in range(len(iteration_folders) - 1): # We don't want to iterate the last one.\n",
    "    current_folder = iteration_folders[folder]\n",
    "    next_folder = iteration_folders[folder + 1]\n",
    "   \n",
    "    print(f\"Working in folder: {current_folder}\")\n",
    "    raster_list = get_raster_file_list(current_folder)\n",
    "\n",
    "    all_pairs_path_list = generate_consecutive_pairs(raster_list)\n",
    "    for raster_pair in all_pairs_path_list[:]:\n",
    "        compare_rasters_dask(raster_pair, next_folder, rule_values_list = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reverse the folder list\"\"\"\n",
    "#This is done separated so we don't do re-reverse at re-executing\n",
    "iteration_folders.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create the aggregated table and build it\"\"\"\n",
    "aggregated_df = process_folders(iteration_folders[:])\n",
    "aggregated_df = add_name_columns_to_dataframe(aggregated_df, names_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build the final table\"\"\"\n",
    "final_raster = get_raster_file_list(iteration_folders[0])[0] # There is only one file\n",
    "vectorize_raster(final_raster, output_path, aggregated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>2010_1</th>\n",
       "      <th>2015_2</th>\n",
       "      <th>2015_3</th>\n",
       "      <th>2020_4</th>\n",
       "      <th>2015_5</th>\n",
       "      <th>2020_6</th>\n",
       "      <th>2020_7</th>\n",
       "      <th>2025_8</th>\n",
       "      <th>2010_1txt</th>\n",
       "      <th>2015_2txt</th>\n",
       "      <th>2015_3txt</th>\n",
       "      <th>2020_4txt</th>\n",
       "      <th>2015_5txt</th>\n",
       "      <th>2020_6txt</th>\n",
       "      <th>2020_7txt</th>\n",
       "      <th>2025_8txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aeropuertos</td>\n",
       "      <td>Arbustal abierto esclerÃ³filo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Aeropuertos</td>\n",
       "      <td>Arbustal abierto esclerÃ³filo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aeropuertos</td>\n",
       "      <td>Arbustal abierto esclerÃ³filo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bosque abierto alto</td>\n",
       "      <td>Arbustal abierto mesÃ³filo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Arenales</td>\n",
       "      <td>Bosque abierto alto</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Aeropuertos</td>\n",
       "      <td>Arbustal abierto mesÃ³filo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arbustal</td>\n",
       "      <td>AlgodÃ³n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afloramientos rocosos</td>\n",
       "      <td>Bosque abierto alto de tierra firme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aeropuerto con infraestructura asociada</td>\n",
       "      <td>Arracachal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arbustal</td>\n",
       "      <td>AjonjolÃ­</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>262</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Arbustal</td>\n",
       "      <td>AjonjolÃ­</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arbustal</td>\n",
       "      <td>AjonjolÃ­</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows Ã 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  2010_1  2015_2  2015_3  2020_4  2015_5  2020_6  2020_7  2025_8  \\\n",
       "0      1     0.0     0.0     0.0     0.0     0.0     0.0     3.0    12.0   \n",
       "1     64     3.0    12.0     0.0     0.0     0.0     0.0     3.0    12.0   \n",
       "2      2    19.0    13.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3      5    15.0    19.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4      7     3.0    13.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "..   ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "258  248     0.0     0.0     0.0     0.0     0.0     0.0    10.0     8.0   \n",
       "259  250     0.0     0.0     0.0     0.0     0.0     0.0     4.0    20.0   \n",
       "260  251     0.0     0.0     0.0     0.0     0.0     0.0     1.0    16.0   \n",
       "261  257     0.0     0.0     0.0     0.0     0.0     0.0    10.0     7.0   \n",
       "262  262    10.0     7.0     0.0     0.0     0.0     0.0    10.0     7.0   \n",
       "\n",
       "               2010_1txt                     2015_2txt 2015_3txt 2020_4txt  \\\n",
       "0                    NaN                           NaN       NaN       NaN   \n",
       "1            Aeropuertos  Arbustal abierto esclerÃ³filo       NaN       NaN   \n",
       "2    Bosque abierto alto     Arbustal abierto mesÃ³filo       NaN       NaN   \n",
       "3               Arenales           Bosque abierto alto       NaN       NaN   \n",
       "4            Aeropuertos     Arbustal abierto mesÃ³filo       NaN       NaN   \n",
       "..                   ...                           ...       ...       ...   \n",
       "258                  NaN                           NaN       NaN       NaN   \n",
       "259                  NaN                           NaN       NaN       NaN   \n",
       "260                  NaN                           NaN       NaN       NaN   \n",
       "261                  NaN                           NaN       NaN       NaN   \n",
       "262             Arbustal                      AjonjolÃ­       NaN       NaN   \n",
       "\n",
       "    2015_5txt 2020_6txt                                2020_7txt  \\\n",
       "0         NaN       NaN                              Aeropuertos   \n",
       "1         NaN       NaN                              Aeropuertos   \n",
       "2         NaN       NaN                                      NaN   \n",
       "3         NaN       NaN                                      NaN   \n",
       "4         NaN       NaN                                      NaN   \n",
       "..        ...       ...                                      ...   \n",
       "258       NaN       NaN                                 Arbustal   \n",
       "259       NaN       NaN                    Afloramientos rocosos   \n",
       "260       NaN       NaN  Aeropuerto con infraestructura asociada   \n",
       "261       NaN       NaN                                 Arbustal   \n",
       "262       NaN       NaN                                 Arbustal   \n",
       "\n",
       "                               2025_8txt  \n",
       "0           Arbustal abierto esclerÃ³filo  \n",
       "1           Arbustal abierto esclerÃ³filo  \n",
       "2                                    NaN  \n",
       "3                                    NaN  \n",
       "4                                    NaN  \n",
       "..                                   ...  \n",
       "258                              AlgodÃ³n  \n",
       "259  Bosque abierto alto de tierra firme  \n",
       "260                           Arracachal  \n",
       "261                             AjonjolÃ­  \n",
       "262                             AjonjolÃ­  \n",
       "\n",
       "[263 rows x 17 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_forge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
